{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## AI Inference from many video files\n",
    "This notebook is an example how to use DeGirum PySDK to do AI inference of multiple video streams from video files multiplexing frames. This example demonstrates lowest possible and stable AI inference latency while maintaining decent throughput. This is achieved by using synchronous prediction mode and video decoding offloaded into separate thread.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify video file names, model name, and other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video files to process\n",
    "input_filenames = [ \n",
    "    \"./images/Traffic.mp4\", \\\n",
    "    \"./images/Traffic.mp4\", \\\n",
    "    \"./images/Traffic.mp4\", \\\n",
    "    \"./images/Traffic.mp4\" \\\n",
    "]\n",
    "\n",
    "model_name = \"mobilenet_v2_ssd_coco--300x300_quant_n2x_orca1_1\" # model to be used for inference\n",
    "\n",
    "# options:\n",
    "offload_preprocessing = True # True to do image preprocessing outside of inference call\n",
    "do_image_compression = True # True to do JPEG compression before sending image for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849e24d-9564-48d2-86d5-c74e4f8a2c37",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "target = dg.CLOUD # <-- on the Cloud Platform\n",
    "# target = mytools.get_ai_server_hostname() # <-- on AI Server deployed in your LAN\n",
    "# target = dg.LOCAL # <-- on ORCA accelerator installed on this computer\n",
    "\n",
    "# connect to AI inference engine getting zoo URL and token from env.ini file\n",
    "zoo = dg.connect(target, mytools.get_cloud_zoo_url(), mytools.get_token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6d38b-d22a-45ab-910d-cf3d4f2dd9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5603895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy, time, threading, queue\n",
    "from contextlib import ExitStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stream multiplexing source:\n",
    "# it reads frames from given video files in round robin fashion\n",
    "# and puts them into given queue.\n",
    "# If offload_preprocessing is enabled, it also performs image resizing\n",
    "def mux_source(streams, frame_queue, model):\n",
    "    \n",
    "    phase = 0 # stream multiplexing phase counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = streams[phase].read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if offload_preprocessing:\n",
    "            # do image resizing\n",
    "            frame = model._preprocessor.forward(frame)[0]\n",
    "            \n",
    "        frame_queue.put((frame, phase))\n",
    "\n",
    "        phase = (phase + 1) % len(streams) # advance mux phase\n",
    "\n",
    "    frame_queue.put(None) # send poison pill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zoo.load_model(model_name) as model, ExitStack() as stack:\n",
    "    # create model object in `with` block to avoid server disconnections on each frame inference\n",
    "\n",
    "    model.input_image_format = \"JPEG\" if do_image_compression else \"RAW\"\n",
    "    model.measure_time = True\n",
    "    \n",
    "    # open video streams\n",
    "    streams = [stack.enter_context(mytools.open_video_stream(fn)) for fn in input_filenames]\n",
    "  \n",
    "    frame_queue = queue.Queue(maxsize=10) # queue to enqueue frames\n",
    "    start_times = [] # list of frame starting times\n",
    "    end_times = [] # list of frame result receiving times\n",
    "    \n",
    "    # start frame retrieving thread\n",
    "    mux_tread = threading.Thread(target=mux_source, args=(streams, frame_queue, model))\n",
    "    mux_tread.start()\n",
    "\n",
    "    # initialize progress indicator\n",
    "    steps = min([stream.get(cv2.CAP_PROP_FRAME_COUNT) for stream in streams])\n",
    "    progress = mytools.Progress(steps * len(streams))        \n",
    "    \n",
    "    # inference loop\n",
    "    start_time=time.time()\n",
    "    while True:\n",
    "        # get frame from queue\n",
    "        frame = frame_queue.get()\n",
    "        if frame is None:\n",
    "            break # got poison pill: end loop\n",
    "        \n",
    "        # do inference and record times\n",
    "        start_times.append(time.time())\n",
    "        res = model(frame[0])\n",
    "        end_times.append(time.time())\n",
    "        \n",
    "        progress.step()\n",
    " \n",
    "    mux_tread.join()\n",
    "        \n",
    "    # print time statistics\n",
    "    for s in model.time_stats().items():\n",
    "        print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process latency times\n",
    "end_times = numpy.array(end_times)\n",
    "start_times = numpy.array(start_times)\n",
    "latency_times_ms = (end_times - start_times) * 1000\n",
    "\n",
    "print(\"\\nLatency Histogram\")\n",
    "latency_hist = numpy.histogram(latency_times_ms)\n",
    "for hval, bin in zip(latency_hist[0], latency_hist[1]):\n",
    "    print(f\"{bin:4.0f} ms:     {hval:4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bd12a-12c2-4aa8-b51e-cfa10021dca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
