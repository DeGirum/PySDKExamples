{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875ec442-4e4c-45a9-8787-ce6726e728a0",
   "metadata": {},
   "source": [
    "# Depth estimation with DepthAnything and DeGirum PySDK\n",
    "\n",
    "[Depth Anything](https://depth-anything.github.io/) is a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, this project aims to build a simple yet powerful foundation model dealing with any images under any circumstances.\n",
    "The framework of Depth Anything is shown below. it adopts a standard pipeline to unleashing the power of large-scale unlabeled images. \n",
    "\n",
    "More details about model can be found in [project web page](https://depth-anything.github.io/), [paper](https://arxiv.org/abs/2401.10891) and official [repository](https://github.com/LiheYoung/Depth-Anything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb00df-3b4e-4854-b77f-bf00371d2b93",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure PySDK and degirum-tools is installed\n",
    "!pip show degirum || pip install -q degirum\n",
    "!pip show degirum-tools || pip install -q degirum-tools\n",
    "!pip show numpy || pip install -q numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d02de-a2ed-425c-8741-bedc20c57db4",
   "metadata": {},
   "source": [
    "There are 3 available models in repository depends on VIT encoder size: \n",
    "* Depth-Anything-ViT-Small\n",
    "* Depth-Anything-ViT-Base\n",
    "* Depth-Anything-ViT-Large\n",
    "\n",
    "We will use `Depth-Anything-ViT-Small`, but the same steps for running model are applicable for other models from DepthAnything family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e36c76",
   "metadata": {},
   "source": [
    "## Run inference on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools as dgtools\n",
    "from depth_postprocessor import DepthResults\n",
    "\n",
    "hw_location = dg.CLOUD\n",
    "model_float_name = 'vits14_depth_anything--518x518_float_openvino_cpu_2'\n",
    "model_quant_name = 'vits14_depth_anything--518x518_quant_openvino_cpu_1'\n",
    "model_zoo_url = 'https://cs.degirum.com/degirum/openvino_demos'\n",
    "zoo = dg.connect(hw_location, model_zoo_url, dgtools.get_token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d46ff0",
   "metadata": {},
   "source": [
    "## Running float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = zoo.load_model(model_float_name, image_backend='pil', input_image_format=\"RAW\", measure_time=True)\n",
    "\n",
    "DepthResults.color_map = \"inferno\"\n",
    "model.custom_postprocessor = DepthResults\n",
    "\n",
    "results_float = model(\"furseal.png\")\n",
    "# Display smaller version in notebook\n",
    "results_float.image_overlay.resize((results_float.image_overlay.size[0] // 4, results_float.image_overlay.size[1] // 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1113d15",
   "metadata": {},
   "source": [
    "## Compare to quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da2d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "model = zoo.load_model(model_quant_name, image_backend='pil', input_image_format=\"RAW\", measure_time=True)\n",
    "\n",
    "DepthResults.color_map = \"inferno\"\n",
    "model.custom_postprocessor = DepthResults\n",
    "\n",
    "results_quant = model(\"furseal.png\")\n",
    "\n",
    "# Stack float and quant images side by side.\n",
    "new_size = tuple(map(lambda x: int(x / 2), results_quant.image.size))\n",
    "compound_image = Image.new('RGB', (results_quant.image.size[0], new_size[1]))\n",
    "compound_image.paste(results_float.image_overlay.resize(new_size))\n",
    "compound_image.paste(results_quant.image_overlay.resize(new_size), (new_size[0], 0))\n",
    "# Label images\n",
    "draw = ImageDraw.Draw(compound_image)\n",
    "font = ImageFont.load_default(100)\n",
    "draw.text((20, 10), \"Float\", (255, 255, 255), font)\n",
    "draw.text((new_size[0] + 20, 10), \"Quant\", (255, 255, 255), font)\n",
    "compound_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d506e3",
   "metadata": {},
   "source": [
    "### Compare inference time of the FP16 and INT8 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53128feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Float inference time: {results_float.results[0]['Timing']['CoreInferenceDuration_ms']}\")\n",
    "print(f\"Quant inference time: {results_quant.results[0]['Timing']['CoreInferenceDuration_ms']}\")\n",
    "print(f\"Speedup: {results_float.results[0]['Timing']['CoreInferenceDuration_ms'] / results_quant.results[0]['Timing']['CoreInferenceDuration_ms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c497cd5-51ca-4ead-912a-cd235f33d61e",
   "metadata": {},
   "source": [
    "### Run inference on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce82adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Video source: https://www.youtube.com/watch?v=fu1xcQdJRws (Public Domain)\n",
    "video_source = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/Coco%20Walking%20in%20Berkeley.mp4\"\n",
    "\n",
    "# Change backend back to opencv for compatibility with dgstreams\n",
    "model = zoo.load_model(model_float_name, image_backend='opencv', input_image_format=\"RAW\")\n",
    "DepthResults.color_map = \"viridis\"\n",
    "model.custom_postprocessor = DepthResults\n",
    "\n",
    "with dgtools.Display(\"AI Camera\") as display:\n",
    "    # AI prediction loop\n",
    "    # Press 'x' or 'q' to stop\n",
    "    # Drag zone by left mouse button to move zone\n",
    "    # Drag zone corners by right mouse button to adjust zone shape\n",
    "    for inference_result in dgtools.predict_stream(\n",
    "        model, video_source\n",
    "    ):\n",
    "        # Stack video and depth map side by side.\n",
    "        h, w, c = inference_result.image.shape\n",
    "        compound_image = np.zeros((h, w * 2, c), dtype=np.uint8)\n",
    "        compound_image[:h,:w] = inference_result.image\n",
    "        compound_image[:h, w:] = inference_result.image_overlay\n",
    "\n",
    "        display.show(compound_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/depth-anything/depth-anything.gif?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Depth Estimation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
