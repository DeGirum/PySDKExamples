{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d7e4eb",
   "metadata": {},
   "source": [
    "# Vehicle Detection And Recognition with OpenVINO™\n",
    "\n",
    "This tutorial demonstrates how to use two pre-trained models from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo): [vehicle-detection-0200](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/vehicle-detection-0200) for object detection and [vehicle-attributes-recognition-barrier-0039](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/vehicle-attributes-recognition-barrier-0039) for image classification. Using these models, you will detect vehicles from raw images and recognize attributes of detected vehicles.\n",
    "![flowchart](https://user-images.githubusercontent.com/47499836/157867076-9e997781-f9ef-45f6-9a51-b515bbf41048.png)\n",
    "\n",
    "As a result, you can get:\n",
    "\n",
    "![result](https://user-images.githubusercontent.com/47499836/157867020-99738b30-62ca-44e2-8d9e-caf13fb724ed.png)\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Imports](#Imports)\n",
    "- [Download Models](#Download-Models)\n",
    "- [Load Models](#Load-Models)\n",
    "    - [Get attributes from model](#Get-attributes-from-model)\n",
    "    - [Helper function](#Helper-function)\n",
    "    - [Read and display a test image](#Read-and-display-a-test-image)\n",
    "- [Use the Detection Model to Detect Vehicles](#Use-the-Detection-Model-to-Detect-Vehicles)\n",
    "    - [Detection Processing](#Detection-Processing)\n",
    "    - [Recognize vehicle attributes](#Recognize-vehicle-attributes)\n",
    "        - [Recognition processing](#Recognition-processing)\n",
    "    - [Combine two models](#Combine-two-models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cae98b",
   "metadata": {},
   "source": [
    "## Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "\n",
    "if platform.system() != \"Windows\":\n",
    "    %pip install -q \"matplotlib>=3.4\"\n",
    "else:\n",
    "    %pip install -q \"matplotlib>=3.4,<3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openvino as ov\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    filename=\"notebook_utils.py\",\n",
    ")\n",
    "\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e7312",
   "metadata": {},
   "source": [
    "## Download Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Download pretrained models from https://storage.openvinotoolkit.org/repositories/open_model_zoo. If the model is already downloaded, this step is skipped.\n",
    "\n",
    "> **Note**: To change the model, replace the name of the model in the code below, for example to `\"vehicle-detection-0201\"` or `\"vehicle-detection-0202\"`. Keep in mind that they support different image input sizes in detection. Also, you can change the recognition model to `\"vehicle-attributes-recognition-barrier-0042\"`. They are trained from different deep learning frames. Therefore, if you want to change the precision, you need to modify the precision value in `\"FP32\"`, `\"FP16\"`, and `\"FP16-INT8\"`. A different type has a different model size and a precision value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = Path(\"model\")\n",
    "# The name of the model from Open Model Zoo.\n",
    "detection_model_name = \"vehicle-detection-0200\"\n",
    "recognition_model_name = \"vehicle-attributes-recognition-barrier-0039\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8)\n",
    "precision = \"FP32\"\n",
    "\n",
    "base_model_url = \"https://storage.openvinotoolkit.org/repositories/open_model_zoo/2023.0/models_bin/1\"\n",
    "\n",
    "# Check if the model exists.\n",
    "detection_model_url = (\n",
    "    f\"{base_model_url}/{detection_model_name}/{precision}/{detection_model_name}.xml\"\n",
    ")\n",
    "recognition_model_url = (\n",
    "    f\"{base_model_url}/{recognition_model_name}/{precision}/{recognition_model_name}.xml\"\n",
    ")\n",
    "detection_model_path = (base_model_dir / detection_model_name).with_suffix('.xml')\n",
    "recognition_model_path = (base_model_dir / recognition_model_name).with_suffix('.xml')\n",
    "\n",
    "# Download the detection model.\n",
    "if not detection_model_path.exists():\n",
    "    utils.download_file(detection_model_url, detection_model_name + '.xml', base_model_dir)\n",
    "    utils.download_file(detection_model_url.replace('.xml', '.bin'), detection_model_name + '.bin', base_model_dir)\n",
    "# Download the recognition model.\n",
    "if not os.path.exists(recognition_model_path):\n",
    "    utils.download_file(recognition_model_url, recognition_model_name + '.xml', base_model_dir)\n",
    "    utils.download_file(recognition_model_url.replace('.xml', '.bin'), recognition_model_name + '.bin', base_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c678fb",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "This tutorial requires a detection model and a recognition model. After downloading the models, initialize OpenVINO Runtime, and use `read_model()` to read network architecture and weights from `*.xml` and `*.bin` files. Then, compile it with `compile_model()` to the specified device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d0695-dd17-4f90-a62c-bba04fdd3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d874b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime runtime.\n",
    "core = ov.Core()\n",
    "\n",
    "\n",
    "def model_init(model_path: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Read the network and weights from file, load the\n",
    "    model on the CPU and get input and output names of nodes\n",
    "\n",
    "    :param: model: model architecture path *.xml\n",
    "    :retuns:\n",
    "            input_key: Input node network\n",
    "            output_key: Output node network\n",
    "            exec_net: Encoder model network\n",
    "            net: Model network\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the network and corresponding weights from a file.\n",
    "    model = core.read_model(model=model_path)\n",
    "    compiled_model = core.compile_model(model=model, device_name=device.value)\n",
    "    # Get input and output names of nodes.\n",
    "    input_keys = compiled_model.input(0)\n",
    "    output_keys = compiled_model.output(0)\n",
    "    return input_keys, output_keys, compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c97641",
   "metadata": {},
   "source": [
    "### Get attributes from model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Use `input_keys.shape` to get data shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de -> detection\n",
    "# re -> recognition\n",
    "# Detection model initialization.\n",
    "input_key_de, output_keys_de, compiled_model_de = model_init(detection_model_path)\n",
    "# Recognition model initialization.\n",
    "input_key_re, output_keys_re, compiled_model_re = model_init(recognition_model_path)\n",
    "\n",
    "# Get input size - Detection.\n",
    "height_de, width_de = list(input_key_de.shape)[2:]\n",
    "# Get input size - Recognition.\n",
    "height_re, width_re = list(input_key_re.shape)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb050ef3",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The `plt_show()` function is used to show image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_show(raw_image):\n",
    "    \"\"\"\n",
    "    Use matplot to show image inline\n",
    "    raw_image: input image\n",
    "\n",
    "    :param: raw_image:image array\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595c8e4",
   "metadata": {},
   "source": [
    "### Read and display a test image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The input shape of detection model is `[1, 3, 256, 256]`. Therefore, you need to resize the image to `256 x 256`, and expand the batch channel with `expand_dims` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef702517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image.\n",
    "url = \"https://storage.openvinotoolkit.org/data/test_data/images/person-bicycle-car-detection.bmp\"\n",
    "filename = \"cars.jpg\"\n",
    "directory = \"data\"\n",
    "image_file = utils.download_file(\n",
    "    url, filename=filename, directory=directory, show_progress=False, silent=True,timeout=30\n",
    ")\n",
    "assert Path(image_file).exists()\n",
    "\n",
    "# Read the image.\n",
    "image_de = cv2.imread(\"data/cars.jpg\")\n",
    "# Resize it to [3, 256, 256].\n",
    "resized_image_de = cv2.resize(image_de, (width_de, height_de))\n",
    "# Expand the batch channel to [1, 3, 256, 256].\n",
    "input_image_de = np.expand_dims(resized_image_de.transpose(2, 0, 1), 0)\n",
    "# Show the image.\n",
    "plt_show(cv2.cvtColor(image_de, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484b074",
   "metadata": {},
   "source": [
    "## Use the Detection Model to Detect Vehicles\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "![pipline](https://user-images.githubusercontent.com/47499836/157867076-9e997781-f9ef-45f6-9a51-b515bbf41048.png)\n",
    "\n",
    "As shown in the flowchart, images of individual vehicles are sent to the recognition model. First, use `infer` function to get the result.\n",
    "\n",
    "The detection model output has the format `[image_id, label, conf, x_min, y_min, x_max, y_max]`, where:\n",
    "\n",
    "- `image_id` - ID of the image in the batch\n",
    "- `label` - predicted class ID (0 - vehicle)\n",
    "- `conf` - confidence for the predicted class\n",
    "- `(x_min, y_min)` - coordinates of the top left bounding box corner\n",
    "- `(x_max, y_max)` - coordinates of the bottom right bounding box corner\n",
    "\n",
    "Delete unused dims and filter out results that are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2540c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference.\n",
    "boxes = compiled_model_de([input_image_de])[output_keys_de]\n",
    "openvino_detection_result = compiled_model_de([input_image_de])[output_keys_de]\n",
    "# Delete the dim of 0, 1.\n",
    "boxes = np.squeeze(boxes, (0, 1))\n",
    "# Remove zero only boxes.\n",
    "boxes = boxes[~np.all(boxes == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c6523",
   "metadata": {},
   "source": [
    "### Detection Processing\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "With the function below, you change the ratio to the real position in the image and filter out low-confidence results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27eeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images(bgr_image, resized_image, boxes, threshold=0.6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use bounding boxes from detection model to find the absolute car position\n",
    "    \n",
    "    :param: bgr_image: raw image\n",
    "    :param: resized_image: resized image\n",
    "    :param: boxes: detection model returns rectangle position\n",
    "    :param: threshold: confidence threshold\n",
    "    :returns: car_position: car's absolute position\n",
    "    \"\"\"\n",
    "    # Fetch image shapes to calculate ratio\n",
    "    (real_y, real_x), (resized_y, resized_x) = bgr_image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Find the boxes ratio\n",
    "    boxes = boxes[:, 2:]\n",
    "    # Store the vehicle's position\n",
    "    car_position = []\n",
    "    # Iterate through non-zero boxes\n",
    "    for box in boxes:\n",
    "        # Pick confidence factor from last place in array\n",
    "        conf = box[0]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio\n",
    "            # In case that bounding box is found at the top of the image, \n",
    "            # upper box  bar should be positioned a little bit lower to make it visible on image \n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                int(max(corner_position * ratio_y * resized_y, 10)) if idx % 2 \n",
    "                else int(corner_position * ratio_x * resized_x)\n",
    "                for idx, corner_position in enumerate(box[1:])\n",
    "            ]\n",
    "            \n",
    "            car_position.append([x_min, y_min, x_max, y_max])\n",
    "            \n",
    "    return car_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the position of a car.\n",
    "car_position = crop_images(image_de, resized_image_de, boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d5580",
   "metadata": {},
   "source": [
    "### Recognize vehicle attributes\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select one of the detected boxes. Then, crop to an area containing a vehicle to test with the recognition model. Again, you need to resize the input image and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a vehicle to recognize.\n",
    "pos = car_position[0]\n",
    "# Crop the image with [y_min:y_max, x_min:x_max].\n",
    "test_car = image_de[pos[1]:pos[3], pos[0]:pos[2]]\n",
    "# Resize the image to input_size.\n",
    "resized_image_re = cv2.resize(test_car, (width_re, height_re))\n",
    "input_image_re = np.expand_dims(resized_image_re.transpose(2, 0, 1), 0)\n",
    "plt_show(cv2.cvtColor(resized_image_re, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70376f",
   "metadata": {},
   "source": [
    "##### Recognition processing\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The result contains colors of the vehicles (white, gray, yellow, red, green, blue, black) and types of vehicles (car, bus, truck, van). Next, you need to calculate the probability of each attribute. Then, you determine the maximum probability as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2159ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicle_recognition(compiled_model_re, input_size, raw_image):\n",
    "    \"\"\"\n",
    "    Vehicle attributes recognition, input a single vehicle, return attributes\n",
    "    :param: compiled_model_re: recognition net \n",
    "    :param: input_size: recognition input size\n",
    "    :param: raw_image: single vehicle image\n",
    "    :returns: attr_color: predicted color\n",
    "                       attr_type: predicted type\n",
    "    \"\"\"\n",
    "    # An attribute of a vehicle.\n",
    "    colors = ['White', 'Gray', 'Yellow', 'Red', 'Green', 'Blue', 'Black']\n",
    "    types = ['Car', 'Bus', 'Truck', 'Van']\n",
    "    \n",
    "    # Resize the image to input size.\n",
    "    resized_image_re = cv2.resize(raw_image, input_size)\n",
    "    input_image_re = np.expand_dims(resized_image_re.transpose(2, 0, 1), 0)\n",
    "    \n",
    "    # Run inference.\n",
    "    # Predict result.\n",
    "    predict_colors = compiled_model_re([input_image_re])[compiled_model_re.output(1)]\n",
    "    openvino_recognition_result_color = compiled_model_re([input_image_re])[compiled_model_re.output(1)]\n",
    "    # Delete the dim of 2, 3.\n",
    "    predict_colors = np.squeeze(predict_colors, (2, 3))\n",
    "    predict_types = compiled_model_re([input_image_re])[compiled_model_re.output(0)]\n",
    "    openvino_recognition_result_type = compiled_model_re([input_image_re])[compiled_model_re.output(0)]\n",
    "    predict_types = np.squeeze(predict_types, (2, 3))\n",
    "\n",
    "    attr_color, attr_type = (colors[np.argmax(predict_colors)],\n",
    "                             types[np.argmax(predict_types)])\n",
    "    return attr_color, attr_type, openvino_recognition_result_color, openvino_recognition_result_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0df2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_color, attr_type, openvino_recognition_result_color, openvino_recognition_result_type = vehicle_recognition(compiled_model_re, (72, 72), test_car)\n",
    "print(f\"Attributes:{attr_color, attr_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62ae24",
   "metadata": {},
   "source": [
    "## PySDK Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_name: name of the model for running AI inference\n",
    "# image_source: image source for inference\n",
    "#     path to image file\n",
    "#     URL of image\n",
    "#     PIL image object\n",
    "#     numpy array\n",
    "hw_location = \"@cloud\"\n",
    "model_zoo_url = \"https://cs.degirum.com/degirum/timm_gender_model_test\"\n",
    "vehicle_det_model_name = \"vehicle_detection--256x256_float_openvino_cpu_1\"\n",
    "vehicle_rec_model_name = \"vehicle_recognition--72x72_float_openvino_cpu_1\"\n",
    "image_source = \"cars.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "vehicle_det_zoo = dg.connect(hw_location, model_zoo_url, degirum_tools.get_token())\n",
    "vehicle_rec_zoo = dg.connect(hw_location, model_zoo_url, degirum_tools.get_token())\n",
    "# load text detection AI model and text recognition AI model\n",
    "vehicle_det_model = vehicle_det_zoo.load_model(vehicle_det_model_name, input_image_format=\"RAW\")\n",
    "vehicle_rec_model = vehicle_rec_zoo.load_model(vehicle_rec_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02befd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysdk_detection_result = vehicle_det_model(image_source).results[0][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a4b03",
   "metadata": {},
   "source": [
    "### Comparing Detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(openvino_detection_result, pysdk_detection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff36586",
   "metadata": {},
   "outputs": [],
   "source": [
    "pysdk_boxes = np.squeeze(pysdk_detection_result, (0, 1))\n",
    "# Remove zero only boxes.\n",
    "pysdk_boxes = pysdk_boxes[~np.all(pysdk_boxes == 0, axis=1)]\n",
    "car_position = crop_images(image_de, resized_image_de, pysdk_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad7daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = car_position[0]\n",
    "# Crop the image with [y_min:y_max, x_min:x_max].\n",
    "test_car = image_de[pos[1]:pos[3], pos[0]:pos[2]]\n",
    "# Resize the image to input_size.\n",
    "rec_H, rec_W = vehicle_rec_model.model_info.InputW[0],vehicle_rec_model.model_info.InputC[0]\n",
    "resized_image_re = cv2.resize(test_car, (rec_H, rec_W))\n",
    "input_image_re = np.expand_dims(resized_image_re.transpose(2, 0, 1), 0)\n",
    "input_image_re = input_image_re.astype(np.float32)  \n",
    "\n",
    "pysdk_recognition_result = vehicle_rec_model(input_image_re)\n",
    "pysdk_recognition_result_color = pysdk_recognition_result.results[1][\"data\"]\n",
    "pysdk_recognition_result_type = pysdk_recognition_result.results[0][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1237a22",
   "metadata": {},
   "source": [
    "### Comparing Recognition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3939f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(pysdk_recognition_result_color, openvino_recognition_result_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(pysdk_recognition_result_type, openvino_recognition_result_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69d44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/47499836/163544861-fa2ad64b-77df-4c16-b065-79183e8ed964.png",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Object Detection",
     "Image Classification"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
