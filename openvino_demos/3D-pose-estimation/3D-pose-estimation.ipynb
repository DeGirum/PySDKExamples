{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live 3D Human Pose Estimation with OpenVINO\n",
    "This notebook demonstrates live 3D Human Pose Estimation with OpenVINO via a webcam. We utilize the model [human-pose-estimation-3d-0001](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/human-pose-estimation-3d-0001) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify where you want to run your inferences, model_zoo_url, model name for 3D pose estimation and video source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_name: name of the model for running AI inference\n",
    "# image_source: image source for inference\n",
    "#     path to image file\n",
    "#     URL of image\n",
    "#     PIL image object\n",
    "#     numpy array\n",
    "\n",
    "hw_location = \"@local\"\n",
    "model_zoo_url = \"https://cs.degirum.com/degirum/openvino_demos\"\n",
    "pose_3d_model_name = \"human_pose_estimation--256x448_float_openvino_cpu_1\"\n",
    "video_source = \"https://github.com/intel-iot-devkit/sample-videos/raw/master/face-demographics-walking.mp4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg,degirum_tools\n",
    "import numpy as np\n",
    "import sys, cv2, time\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "sys.path.append(\"./engine\")\n",
    "import engine.engine3js as engine\n",
    "from engine.parse_poses import parse_poses\n",
    "from pose_3d_preprocessor import Pose3DPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_3d_zoo = dg.connect(hw_location, model_zoo_url, degirum_tools.get_token())\n",
    "pose_3d_model = pose_3d_zoo.load_model(pose_3d_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D edge index array\n",
    "body_edges = np.array(\n",
    "    [\n",
    "        [0, 1],\n",
    "        [0, 9],\n",
    "        [9, 10],\n",
    "        [10, 11],  # neck - r_shoulder - r_elbow - r_wrist\n",
    "        [0, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],  # neck - l_shoulder - l_elbow - l_wrist\n",
    "        [1, 15],\n",
    "        [15, 16],  # nose - l_eye - l_ear\n",
    "        [1, 17],\n",
    "        [17, 18],  # nose - r_eye - r_ear\n",
    "        [0, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],  # neck - l_hip - l_knee - l_ankle\n",
    "        [0, 12],\n",
    "        [12, 13],\n",
    "        [13, 14],  # neck - r_hip - r_knee - r_ankle\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "body_edges_2d = np.array(\n",
    "    [\n",
    "        [0, 1],  # neck - nose\n",
    "        [1, 16],\n",
    "        [16, 18],  # nose - l_eye - l_ear\n",
    "        [1, 15],\n",
    "        [15, 17],  # nose - r_eye - r_ear\n",
    "        [0, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],  # neck - l_shoulder - l_elbow - l_wrist\n",
    "        [0, 9],\n",
    "        [9, 10],\n",
    "        [10, 11],  # neck - r_shoulder - r_elbow - r_wrist\n",
    "        [0, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],  # neck - l_hip - l_knee - l_ankle\n",
    "        [0, 12],\n",
    "        [12, 13],\n",
    "        [13, 14],  # neck - r_hip - r_knee - r_ankle\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def draw_poses(frame, poses_2d, scaled_frame, use_popup):\n",
    "    \"\"\"\n",
    "    Draw 2D pose overlays on the image to visualize estimated poses.\n",
    "    Joints are drawn as circles and limbs are drawn as lines.\n",
    "\n",
    "    :param frame: the input image\n",
    "    :param poses_2d: array of human joint pairs\n",
    "    \"\"\"\n",
    "    for pose in poses_2d:\n",
    "        pose = np.array(pose[0:-1]).reshape((-1, 3)).transpose()\n",
    "        was_found = pose[2] > 0\n",
    "\n",
    "        pose[0], pose[1] = (\n",
    "            pose[0] * frame.shape[1] / scaled_frame.shape[1],\n",
    "            pose[1] * frame.shape[0] / scaled_frame.shape[0],\n",
    "        )\n",
    "        # Draw joints.\n",
    "        for edge in body_edges_2d:\n",
    "            if was_found[edge[0]] and was_found[edge[1]]:\n",
    "                cv2.line(\n",
    "                    frame,\n",
    "                    tuple(pose[0:2, edge[0]].astype(np.int32)),\n",
    "                    tuple(pose[0:2, edge[1]].astype(np.int32)),\n",
    "                    (255, 255, 0),\n",
    "                    4,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "        # Draw limbs.\n",
    "        for kpt_id in range(pose.shape[1]):\n",
    "            if pose[2, kpt_id] != -1:\n",
    "                cv2.circle(\n",
    "                    frame,\n",
    "                    tuple(pose[0:2, kpt_id].astype(np.int32)),\n",
    "                    3,\n",
    "                    (0, 255, 255),\n",
    "                    -1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_H, inp_W = pose_3d_model.model_info.InputW[0],pose_3d_model.model_info.InputC[0]\n",
    "inp_H, inp_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 100  # Maximum number of frames to read from video, set to 0 for all frames.\n",
    "sample_duration = 16\n",
    "use_popup=False\n",
    "with degirum_tools.open_video_stream(video_source) as video_stream:\n",
    "    w, h, fps = degirum_tools.get_video_stream_properties(video_stream)\n",
    "    fps = 30\n",
    "    if num_frames == 0:\n",
    "        total_frames = video_stream.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    else:\n",
    "        total_frames = num_frames\n",
    "    counter = 0\n",
    "    frames = []\n",
    "    focal_length = -1  # default\n",
    "    stride = 8\n",
    "    skeleton_set = None\n",
    "    progress = degirum_tools.Progress(total_frames)\n",
    "    if use_popup:\n",
    "        title = \"Press ESC to Exit\"\n",
    "        cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)  \n",
    "        \n",
    "    for i, frame in enumerate(degirum_tools.video_source(video_stream)):\n",
    "        if i == 0:\n",
    "            resize_scale = 450 / frame.shape[1]\n",
    "            windows_width = int(frame.shape[1] * resize_scale)\n",
    "            windows_height = int(frame.shape[0] * resize_scale)\n",
    "\n",
    "            # use visualization library\n",
    "            engine3D = engine.Engine3js(\n",
    "                grid=True, axis=True, view_width=windows_width, view_height=windows_height\n",
    "            )\n",
    "\n",
    "            if use_popup:\n",
    "                # display the 3D human pose in this notebook, and origin frame in popup window\n",
    "                display(engine3D.renderer)\n",
    "                title = \"Press ESC to Exit\"\n",
    "                cv2.namedWindow(title, cv2.WINDOW_KEEPRATIO | cv2.WINDOW_AUTOSIZE)\n",
    "            else:\n",
    "                # set the 2D image box, show both human pose and image in the notebook\n",
    "                imgbox = widgets.Image(\n",
    "                    format=\"jpg\", height=windows_height, width=windows_width\n",
    "                )\n",
    "                display(widgets.HBox([engine3D.renderer, imgbox]))\n",
    "\n",
    "            skeleton = engine.Skeleton(body_edges=body_edges)\n",
    "\n",
    "        if i == total_frames:\n",
    "            break\n",
    "\n",
    "        resized_frame, scaled_frame = Pose3DPreprocessor(inp_H, inp_W).load_frame(frame)\n",
    "        if focal_length < 0:  # Focal length is unknown\n",
    "            focal_length = np.float32(0.8 * scaled_frame.shape[1])\n",
    "\n",
    "        # inference start\n",
    "        start_time = time.time()\n",
    "        # get results\n",
    "        res = pose_3d_model(scaled_frame)\n",
    "        inference_result = (res.results[0][\"data\"][0],res.results[1][\"data\"][0],res.results[2][\"data\"][0])\n",
    "        # Process the point to point coordinates of the data\n",
    "        poses_3d, poses_2d = parse_poses(\n",
    "            inference_result, 1, stride, focal_length, True\n",
    "        )\n",
    "        if len(poses_3d) > 0:\n",
    "            # From here, you can rotate the 3D point positions using the function \"draw_poses\",\n",
    "            # or you can directly make the correct mapping below to properly display the object image on the screen\n",
    "            poses_3d_copy = poses_3d.copy()\n",
    "            x = poses_3d_copy[:, 0::4]\n",
    "            y = poses_3d_copy[:, 1::4]\n",
    "            z = poses_3d_copy[:, 2::4]\n",
    "            poses_3d[:, 0::4], poses_3d[:, 1::4], poses_3d[:, 2::4] = (\n",
    "                -z + np.ones(poses_3d[:, 2::4].shape) * 200,\n",
    "                -y + np.ones(poses_3d[:, 2::4].shape) * 100,\n",
    "                -x,\n",
    "            )\n",
    "\n",
    "            poses_3d = poses_3d.reshape(poses_3d.shape[0], 19, -1)[:, :, 0:3]\n",
    "            people = skeleton(poses_3d=poses_3d)\n",
    "\n",
    "            try:\n",
    "                engine3D.scene_remove(skeleton_set)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            engine3D.scene_add(people)\n",
    "            skeleton_set = people\n",
    "\n",
    "            # draw 2D\n",
    "            frame = draw_poses(frame, poses_2d, resized_frame, use_popup)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                engine3D.scene_remove(skeleton_set)\n",
    "                skeleton_set = None\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "        if use_popup:\n",
    "            cv2.imshow(title, frame)\n",
    "            key = cv2.waitKey(1)\n",
    "            # escape = 27, use ESC to exit\n",
    "            if key == 27:\n",
    "                break\n",
    "        else:\n",
    "            # encode numpy array to jpg\n",
    "            imgbox.value = cv2.imencode(\n",
    "                \".jpg\",\n",
    "                frame,\n",
    "                params=[cv2.IMWRITE_JPEG_QUALITY, 90],\n",
    "            )[1].tobytes()\n",
    "\n",
    "        engine3D.renderer.render(engine3D.scene, engine3D.cam)         \n",
    "        progress.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
