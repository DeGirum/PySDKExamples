{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with OpenVINO and PySDK\n",
    "\n",
    "In this notebook, the Segmenter OpenVINO and PySDK models are compared for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# clone Segmenter repo\n",
    "if not Path(\"segmenter\").exists():\n",
    "    !git clone https://github.com/rstrudel/segmenter\n",
    "else:\n",
    "    print(\"Segmenter repo already cloned\")\n",
    "\n",
    "# include path to Segmenter repo to use its functions\n",
    "sys.path.append(\"./segmenter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing requirements\n",
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "%pip install -r segmenter/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# Fetch the notebook utils script from the openvino_notebooks repo\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/master/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "from notebook_utils import download_file, load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download config and pretrained model weights\n",
    "# here we use tiny model, there are also better but larger models available in repository\n",
    "WEIGHTS_LINK = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/segmenter/checkpoints/ade20k/seg_tiny_mask/checkpoint.pth\"\n",
    "CONFIG_LINK = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/segmenter/checkpoints/ade20k/seg_tiny_mask/variant.yml\"\n",
    "\n",
    "MODEL_DIR = Path(\"model/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(WEIGHTS_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "download_file(CONFIG_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "\n",
    "WEIGHT_PATH = MODEL_DIR / \"checkpoint.pth\"\n",
    "CONFIG_PATH = MODEL_DIR / \"variant.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "PyTorch models are usually an instance of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary containing model weights.\n",
    "Typical steps to get the model are therefore:\n",
    "\n",
    "1. Create an instance of the model class\n",
    "2. Load checkpoint state dict, which contains pre-trained model weights\n",
    "3. Turn the model to evaluation mode, to switch some operations to inference mode\n",
    "\n",
    "We will now use already provided helper functions from repository to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmenter.segm.model.factory import load_model\n",
    "\n",
    "pytorch_model, config = load_model(WEIGHT_PATH)\n",
    "# put model into eval mode, to set it for inference\n",
    "pytorch_model.eval()\n",
    "print(\"PyTorch model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load normalization settings from config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from segmenter.segm.data.utils import STATS\n",
    "# load normalization name, in our case \"vit\" since we are using transformer\n",
    "normalization_name = config[\"dataset_kwargs\"][\"normalization\"]\n",
    "# load normalization params, mean and std from STATS\n",
    "normalization = STATS[normalization_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing preprocessing, inference support functions, and visualization functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now we will define utility functions for preprocessing, inferencing, and visualizing the results.\n",
    "\n",
    "### Preprocessing\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Inference input is tensor with shape `[1, 3, H, W]` in `B, C, H, W` format, where:\n",
    "\n",
    "* `B` - batch size (in our case 1, as we are just adding 1 with unsqueeze)\n",
    "* `C` - image channels (in our case RGB - 3)\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "Splitting to batches is done inside inference, so we don't need to split the image in preprocessing.\n",
    "\n",
    "Model expects images in RGB channels format, scaled to [0, 1] range and normalized with given mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import math\n",
    "\n",
    "def resize(im, smaller_size):\n",
    "    \"\"\"\n",
    "    Resize image bilinearly to make shorter side at least as long as the dimension provided.\n",
    "    \"\"\"\n",
    "    h, w = im.shape[2:]\n",
    "    if h < w:\n",
    "        ratio = w / h\n",
    "        h_res, w_res = smaller_size, ratio * smaller_size\n",
    "    else:\n",
    "        ratio = h / w\n",
    "        h_res, w_res = ratio * smaller_size, smaller_size\n",
    "    if min(h, w) < smaller_size:\n",
    "        im_res = cv2.resize(im, (int(h_res), int(w_res)), interpolation=cv2.INTER_LINEAR)\n",
    "    else:\n",
    "        im_res = im\n",
    "    return im_res\n",
    "\n",
    "def preprocess(im: Image, normalization: dict, window_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess image: scale, normalize, unsqueeze, and resize\n",
    "\n",
    "    :param im: input image\n",
    "    :param normalization: dictionary containing normalization data\n",
    "    :return:\n",
    "            im: processed (scaled and normalized) image\n",
    "    \"\"\"\n",
    "    # change PIL image to NumPy array and scale to [0, 1]\n",
    "    im = np.asarray(im, dtype=np.float32) / 255.0\n",
    "    # normalize by given mean and standard deviation\n",
    "    im -= np.array(normalization[\"mean\"])[None, None, :]\n",
    "    im /= np.array(normalization[\"std\"])[None, None, :]\n",
    "    # HWC -> CHW\n",
    "    im = np.transpose(im, (2, 0, 1))\n",
    "    # change dim from [C, H, W] to [1, C, H, W]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    # resize image to window size by shorter dimension\n",
    "    im = resize(im, window_size)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Support\n",
    "The functions below implement a sliding window methodology for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(im, flip, window_size, window_stride):\n",
    "    \"\"\"\n",
    "    Create a batch of patches from the input image, based on provided window\n",
    "    size and stride.\n",
    "    \"\"\"\n",
    "    B, C, H, W = im.shape\n",
    "    ws = window_size\n",
    "\n",
    "    windows = {\"crop\": [], \"anchors\": []}\n",
    "    h_anchors = np.arange(0, H, window_stride)\n",
    "    w_anchors = np.arange(0, W, window_stride)\n",
    "    h_anchors = [h for h in h_anchors if h < H - ws] + [H - ws]\n",
    "    w_anchors = [w for w in w_anchors if w < W - ws] + [W - ws]\n",
    "    for ha in h_anchors:\n",
    "        for wa in w_anchors:\n",
    "            window = im[:, :, ha : ha + ws, wa : wa + ws]\n",
    "            windows[\"crop\"].append(window)\n",
    "            windows[\"anchors\"].append((ha, wa))\n",
    "    windows[\"flip\"] = flip\n",
    "    windows[\"shape\"] = (H, W)\n",
    "    return windows\n",
    "\n",
    "def bilinear_interpolation(original_img, new_shape):\n",
    "\t\"\"\"\n",
    "\tBilinear interpolation for masks in CHW format.\n",
    "\t\"\"\"\n",
    "\t#get dimensions of original image\n",
    "\toriginal_img = np.transpose(original_img, (1, 2, 0))\n",
    "\told_h, old_w, c = original_img.shape\n",
    "\tnew_h, new_w = new_shape\n",
    "\t#create an array of the desired shape. \n",
    "\t#We will fill-in the values later.\n",
    "\tresized = np.zeros((new_h, new_w, c), dtype=original_img.dtype)\n",
    "\t#Calculate horizontal and vertical scaling factor\n",
    "\tw_scale_factor = old_w / new_w\n",
    "\th_scale_factor = old_h / new_h\n",
    "\tfor i in range(new_h):\n",
    "\t\tfor j in range(new_w):\n",
    "\t\t\t#map the coordinates back to the original image\n",
    "\t\t\tx = i * h_scale_factor\n",
    "\t\t\ty = j * w_scale_factor\n",
    "\t\t\t#calculate the coordinate values for 4 surrounding pixels.\n",
    "\t\t\tx_floor = math.floor(x)\n",
    "\t\t\tx_ceil = min( old_h - 1, math.ceil(x))\n",
    "\t\t\ty_floor = math.floor(y)\n",
    "\t\t\ty_ceil = min(old_w - 1, math.ceil(y))\n",
    "\n",
    "\t\t\tif (x_ceil == x_floor) and (y_ceil == y_floor):\n",
    "\t\t\t\tq = original_img[int(x), int(y), :]\n",
    "\t\t\telif (x_ceil == x_floor):\n",
    "\t\t\t\tq1 = original_img[int(x), int(y_floor), :]\n",
    "\t\t\t\tq2 = original_img[int(x), int(y_ceil), :]\n",
    "\t\t\t\tq = q1 * (y_ceil - y) + q2 * (y - y_floor)\n",
    "\t\t\telif (y_ceil == y_floor):\n",
    "\t\t\t\tq1 = original_img[int(x_floor), int(y), :]\n",
    "\t\t\t\tq2 = original_img[int(x_ceil), int(y), :]\n",
    "\t\t\t\tq = (q1 * (x_ceil - x)) + (q2\t * (x - x_floor))\n",
    "\t\t\telse:\n",
    "\t\t\t\tv1 = original_img[x_floor, y_floor, :]\n",
    "\t\t\t\tv2 = original_img[x_ceil, y_floor, :]\n",
    "\t\t\t\tv3 = original_img[x_floor, y_ceil, :]\n",
    "\t\t\t\tv4 = original_img[x_ceil, y_ceil, :]\n",
    "\n",
    "\t\t\t\tq1 = v1 * (x_ceil - x) + v2 * (x - x_floor)\n",
    "\t\t\t\tq2 = v3 * (x_ceil - x) + v4 * (x - x_floor)\n",
    "\t\t\t\tq = q1 * (y_ceil - y) + q2 * (y - y_floor)\n",
    "\n",
    "\t\t\tresized[i,j,:] = q\n",
    "\tresized = np.transpose(resized, (2, 0, 1))\n",
    "\treturn resized\n",
    "\n",
    "def merge_windows(windows, window_size, ori_shape):\n",
    "    \"\"\"\n",
    "    Merge all inference result patches to create complete segmentation\n",
    "    mask output of the model.\n",
    "    \"\"\"\n",
    "    ws = window_size\n",
    "    im_windows = windows[\"seg_maps\"]\n",
    "    anchors = windows[\"anchors\"]\n",
    "    C = im_windows[0].shape[0]\n",
    "    H, W = windows[\"shape\"]\n",
    "    flip = windows[\"flip\"]\n",
    "\n",
    "    logit = np.zeros((C, H, W), dtype=np.float32)\n",
    "    count = np.zeros((1, H, W), dtype=np.float32)\n",
    "    for window, (ha, wa) in zip(im_windows, anchors):\n",
    "        logit[:, ha : ha + ws, wa : wa + ws] += window\n",
    "        count[:, ha : ha + ws, wa : wa + ws] += 1\n",
    "    logit = logit / count\n",
    "    logit = bilinear_interpolation(\n",
    "        logit,\n",
    "        ori_shape\n",
    "    )\n",
    "    if flip:\n",
    "        logit = np.flip(logit, axis=2)\n",
    "    result = softmax(logit, axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Function\n",
    "\n",
    "The following code implements the main inference function, utilizing the inference support functions defined above to split the input image into patches, run the model inference on each path, and merge together all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model,\n",
    "    ims,\n",
    "    ims_metas,\n",
    "    ori_shape,\n",
    "    window_size,\n",
    "    window_stride,\n",
    "    batch_size,\n",
    "):\n",
    "    C = model.n_cls\n",
    "    seg_map = np.zeros((C, ori_shape[0], ori_shape[1]))\n",
    "    for im, im_metas in zip(ims, ims_metas):\n",
    "        flip = im_metas[\"flip\"]\n",
    "        windows = sliding_window(im, flip, window_size, window_stride)\n",
    "        crops = np.stack(windows.pop(\"crop\"))[:, 0]\n",
    "        B = len(crops)\n",
    "        WB = batch_size\n",
    "        seg_maps = np.zeros((B, C, window_size, window_size))\n",
    "        for i in range(0, B, WB):\n",
    "            seg_maps[i : i + WB] = model.forward(crops[i : i + WB])\n",
    "        windows[\"seg_maps\"] = seg_maps\n",
    "        im_seg_map = merge_windows(windows, window_size, ori_shape)\n",
    "        seg_map += im_seg_map\n",
    "    seg_map /= len(ims)\n",
    "    return seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Inference output contains labels assigned to each pixel, so the output in our case is `[150, H, W]` in `CL, H, W` format where:\n",
    "\n",
    "* `CL` - number of classes for labels (in our case 150)\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "Since we want to visualize this output, we reduce dimensions to `[1, H, W]` where we keep only class with the highest value as that is the predicted label.\n",
    "We then combine original image with colors corresponding to the inferred labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "sys.path.append(\"./segmenter\")\n",
    "\n",
    "from segmenter.segm.data.utils import IGNORE_LABEL\n",
    "from segmenter.segm.data.ade20k import ADE20K_CATS_PATH\n",
    "\n",
    "def seg_to_rgb(seg, colors):\n",
    "    im = np.zeros((seg.shape[0], seg.shape[1], seg.shape[2], 3), dtype=np.float32)\n",
    "    cls = np.unique(seg)\n",
    "    for cl in cls:\n",
    "        color = colors[int(cl)]\n",
    "        if len(color.shape) > 1:\n",
    "            color = color[0]\n",
    "        im[seg == cl] = color\n",
    "    return im\n",
    "\n",
    "def dataset_cat_description(path, cmap=None):\n",
    "    desc = yaml.load(open(path, \"r\"), Loader=yaml.FullLoader)\n",
    "    colors = {}\n",
    "    names = []\n",
    "    for i, cat in enumerate(desc):\n",
    "        names.append(cat[\"name\"])\n",
    "        if \"color\" in cat:\n",
    "            colors[cat[\"id\"]] = np.array(cat[\"color\"], dtype=np.float32) / 255.0\n",
    "        else:\n",
    "            colors[cat[\"id\"]] = np.array(cmap[cat[\"id\"]], dtype=np.float32)\n",
    "    colors[IGNORE_LABEL] = np.array([0.0, 0.0, 0.0], dtype=np.float32)\n",
    "    return names, colors\n",
    "\n",
    "def apply_segmentation_mask(pil_im: Image, results: np.ndarray) -> Image:\n",
    "    \"\"\"\n",
    "    Combine segmentation masks with the image\n",
    "\n",
    "    :param pil_im: original input image\n",
    "    :param results: tensor containing segmentation masks for each pixel\n",
    "    :return:\n",
    "            pil_blend: image with colored segmentation masks overlay\n",
    "    \"\"\"\n",
    "    cat_names, cat_colors = dataset_cat_description(ADE20K_CATS_PATH)\n",
    "\n",
    "    # 3D array, where each pixel has values for all classes, take index of max as label\n",
    "    seg_map = np.argmax(results, axis=0, keepdims=True)\n",
    "    # transform label id to colors\n",
    "    seg_rgb = seg_to_rgb(seg_map, cat_colors)\n",
    "    seg_rgb = (255 * seg_rgb).astype(np.uint8)\n",
    "    pil_seg = Image.fromarray(seg_rgb[0])\n",
    "\n",
    "    # overlay segmentation mask over original image\n",
    "    pil_blend = Image.blend(pil_im, pil_seg, 0.5).convert(\"RGB\")\n",
    "\n",
    "    return pil_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PyTorch model to OpenVINO Intermediate Representation (IR)\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now that we've verified that the inference of PyTorch model works, we will convert it to OpenVINO IR format.\n",
    "\n",
    "To do this, we first get input dimensions from the model configuration file and create torch dummy input.\n",
    "Input dimensions are in our case `[2, 3, 512, 512]` in `B, C, H, W]` format, where:\n",
    "\n",
    "* `B` - batch size\n",
    "* `C` - image channels (in our case RGB - 3)\n",
    "* `H` - model input image height\n",
    "* `W` - model input image width\n",
    "\n",
    "> Note that H and W are here fixed to 512, as this is required by the model. Resizing is done inside the inference function from the original repository.\n",
    "\n",
    "After that, we use `ov.convert_model` function from PyTorch to convert the model to OpenVINO model,  which is ready to use in Python interface but can also be serialized to OpenVINO IR format for future execution using `ov.save_model`.\n",
    "The process can generate some warnings, but they are not a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import torch\n",
    "\n",
    "# get input sizes from config file\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "image_size = config[\"dataset_kwargs\"][\"image_size\"]\n",
    "\n",
    "# make dummy input with correct shapes obtained from config file\n",
    "dummy_input = torch.randn(batch_size, channels, image_size, image_size)\n",
    "\n",
    "model = ov.convert_model(pytorch_model, example_input=dummy_input, input=([batch_size, channels, image_size, image_size], ))\n",
    "# serialize model for saving IR\n",
    "ov.save_model(model, MODEL_DIR / \"segmenter.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify converted model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To test that model was successfully converted, we can use same inference function from original repository, but we need to make custom class.\n",
    "\n",
    "`SegmenterOV` class contains OpenVINO model, with all attributes and methods required by inference function.\n",
    "This way we don't need to write custom code required to process input when comparing to the PySDK model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image with PIL\n",
    "image = load_image(\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_hollywood.jpg\")\n",
    "# load_image reads the image in BGR format, [:,:,::-1] reshape transfroms it to RGB\n",
    "pil_image = Image.fromarray(image[:,:,::-1])\n",
    "\n",
    "# preprocess image with normalization params loaded in previous steps\n",
    "image = preprocess(pil_image, normalization, 512)\n",
    "\n",
    "# inference function needs some meta parameters, where we specify that we don't flip images in inference mode\n",
    "im_meta = dict(flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SegmenterOV:\n",
    "    \"\"\"\n",
    "    Class containing OpenVINO model with all attributes required to work with inference function.\n",
    "\n",
    "    :param model: compiled OpenVINO model\n",
    "    :type model: CompiledModel\n",
    "    :param output_blob: output blob used in inference\n",
    "    :type output_blob: ConstOutput\n",
    "    :param config: config file containing data about model and its requirements\n",
    "    :type config: dict\n",
    "    :param n_cls: number of classes to be predicted\n",
    "    :type n_cls: int\n",
    "    :param normalization:\n",
    "    :type normalization: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: Path, device:str = \"CPU\"):\n",
    "        \"\"\"\n",
    "        Constructor method.\n",
    "        Initializes OpenVINO model and sets all required attributes\n",
    "\n",
    "        :param model_path: path to model's .xml file, also containing variant.yml\n",
    "        :param device: device string for selecting inference device\n",
    "        \"\"\"\n",
    "        # init OpenVino core\n",
    "        core = ov.Core()\n",
    "        # read model\n",
    "        model_xml = core.read_model(model_path)\n",
    "        self.model = core.compile_model(model_xml, device)\n",
    "        self.output_blob = self.model.output(0)\n",
    "\n",
    "        # load model configs\n",
    "        variant_path = Path(model_path).parent / \"variant.yml\"\n",
    "        with open(variant_path, \"r\") as f:\n",
    "            self.config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # load normalization specs from config\n",
    "        normalization_name = self.config[\"dataset_kwargs\"][\"normalization\"]\n",
    "        self.normalization = STATS[normalization_name]\n",
    "\n",
    "        # load number of classes from config\n",
    "        self.n_cls = self.config[\"net_kwargs\"][\"n_cls\"]\n",
    "\n",
    "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform inference on data and return the result in Tensor format\n",
    "\n",
    "        :param data: input data to model\n",
    "        :return: data inferred by model\n",
    "        \"\"\"\n",
    "        return self.model(data)[self.output_blob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created `SegmenterOV` helper class, we can use it in the inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device from dropdown list for running inference using OpenVINO\n",
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model into SegmenterOV class\n",
    "model_ov = SegmenterOV(MODEL_DIR / \"segmenter.xml\", device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference with same function as in case of PyTorch model from repository\n",
    "results_ov = inference(model=model_ov,\n",
    "                    ims=[image],\n",
    "                    ims_metas=[im_meta],\n",
    "                    ori_shape=image.shape[2:4],\n",
    "                    window_size=model_ov.config[\"inference_kwargs\"][\"window_size\"],\n",
    "                    window_stride=model_ov.config[\"inference_kwargs\"][\"window_stride\"],\n",
    "                    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "converted_blend = apply_segmentation_mask(pil_image, results_ov)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "converted_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PySDK model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We can use the same inference function from the original repository, but for that we need to make a custom class similar to `SegmenterOV`.\n",
    "\n",
    "`SegmenterPySDK` class contains PySDK model, with all attributes and methods required by the inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "\n",
    "target = \"@cloud\"\n",
    "cloud_zoo_url = \"https://cs.degirum.com/degirum/openvino_demos\"\n",
    "cloud_token = \"<your DeGirum Cloud token here>\"\n",
    "\n",
    "zoo = dg.connect(target, cloud_zoo_url, cloud_token)\n",
    "model_loaded = zoo.load_model(\"segmenter--512x512_float_openvino_cpu_1\")\n",
    "model_loaded.input_image_format = \"RAW\"\n",
    "model_loaded.image_backend = \"pil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "class SegmenterPySDK:\n",
    "    \"\"\"\n",
    "    Class containing PySDK model with all attributes required to work with inference function.\n",
    "\n",
    "    :param model: PySDK model\n",
    "    :type model: CompiledModel\n",
    "    :param output_blob: output blob used in inference\n",
    "    :type output_blob: ConstOutput\n",
    "    :param config: config file containing data about model and its requirements\n",
    "    :type config: dict\n",
    "    :param n_cls: number of classes to be predicted\n",
    "    :type n_cls: int\n",
    "    :param normalization:\n",
    "    :type normalization: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        Constructor method.\n",
    "        Initializes PySDK model and sets all required attributes\n",
    "\n",
    "        :param model: PySDK model object\n",
    "        :param device: device string for selecting inference device\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "        # load model configs\n",
    "        variant_path = Path(\"/home/degirum/Desktop/Software_Workspace/OpenVINO Segmentation Demo Notebooks/Segmenter/model/segmenter.xml\").parent / \"variant.yml\"\n",
    "        with open(variant_path, \"r\") as f:\n",
    "            self.config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # load normalization specs from config\n",
    "        normalization_name = self.config[\"dataset_kwargs\"][\"normalization\"]\n",
    "        self.normalization = STATS[normalization_name]\n",
    "\n",
    "        # load number of classes from config\n",
    "        self.n_cls = self.config[\"net_kwargs\"][\"n_cls\"]\n",
    "\n",
    "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform inference on data and return the result in Tensor format\n",
    "\n",
    "        :param data: input data to model\n",
    "        :return: data inferred by model\n",
    "        \"\"\"\n",
    "        return self.model(np.transpose(data, (0,2,3,1)).tobytes()).results[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pysdk = SegmenterPySDK(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference with same function as in case of PyTorch model from repository\n",
    "results_pysdk = inference(model=model_pysdk,\n",
    "                    ims=[image],\n",
    "                    ims_metas=[im_meta],\n",
    "                    ori_shape=image.shape[2:4],\n",
    "                    window_size=model_pysdk.config[\"inference_kwargs\"][\"window_size\"],\n",
    "                    window_stride=model_pysdk.config[\"inference_kwargs\"][\"window_stride\"],\n",
    "                    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "converted_blend = apply_segmentation_mask(pil_image, results_pysdk)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "converted_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare OpenVINO and PySDK results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(results_pysdk, results_ov, rtol=1e-05, atol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get effectively the same results for both the OpenVINO and PySDK models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/61357777/223854308-d1ac4a39-cc0c-4618-9e4f-d9d4d8b991e8.jpg",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image Segmentation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
