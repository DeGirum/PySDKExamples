{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with DeGirum's PySDK using Segmenter\n",
    "\n",
    "Semantic segmentation is a difficult computer vision problem with many applications such as autonomous driving, robotics, augmented reality, and many others.\n",
    "Its goal is to assign labels to each pixel according to the object it belongs to, creating so-called segmentation masks.\n",
    "To properly assign this label, the model needs to consider the local as well as global context of the image.\n",
    "This is where transformers offer their advantage as they work well in capturing global context.\n",
    "\n",
    "Segmenter is based on Vision Transformer working as an encoder, and Mask Transformer working as a decoder.\n",
    "With this configuration, it achieves good results on different datasets such as ADE20K, Pascal Context, and Cityscapes.\n",
    "It works as shown in the diagram below, by taking the image, splitting it into patches, and then encoding these patches.\n",
    "Mask transformer combines encoded patches with class masks and decodes them into a segmentation map as the output, where each pixel has a label assigned to it.\n",
    "\n",
    "![Segmenter diagram](https://github.com/openvinotoolkit/openvino_notebooks/assets/93932510/f57979e7-fd3b-449f-bf01-afe0f965abbc)\n",
    "> Credits for this image go to [original authors of Segmenter](https://github.com/rstrudel/segmenter).\n",
    "\n",
    "More about the model and its details can be found in the following paper:\n",
    "[Segmenter: Transformer for Semantic Segmentation](https://arxiv.org/abs/2105.05633) or in the [repository](https://github.com/rstrudel/segmenter).\n",
    "\n",
    "The following notebook implements the execution of the Segmenter model in OpenVINO runtime using DeGirum's PySDK.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Get and prepare PySDK model](#Get-and-prepare-PySDK-model)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "    - [Loading PySDK model](#Loading-PySDK-model)\n",
    "- [Preparing preprocessing, inference support functions, and visualization functions](#Preparing-preprocessing-inference-support-functions-and-visualization-functions)\n",
    "    - [Preprocessing](#Preprocessing)\n",
    "    - [Inference Support](#Inference-Support)\n",
    "    - [Inference Function](#Inference-Function)\n",
    "    - [Visualization](#Visualization)\n",
    "- [Inference of PySDK model](#Inference-of-PySDK-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how to use Segmenter with PySDK, this notebook consists of the following steps:\n",
    "\n",
    "* Preparing PySDK Segmenter model\n",
    "* Preparing preprocessing and visualization functions\n",
    "* Running inference of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and prepare PySDK model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: degirum_tools\n",
      "Version: 0.9.0\n",
      "Summary: Tools for PySDK\n",
      "Home-page: \n",
      "Author: DeGirum\n",
      "Author-email: \n",
      "License: \n",
      "Location: /home/mehrdad/pysdk/lib/python3.10/site-packages\n",
      "Requires: degirum, ffmpegcv, ipython, numpy, opencv-python, pafy, pillow, psutil, pycocotools, python-dotenv, pyyaml, requests, scipy, youtube-dl\n",
      "Required-by: \n",
      "Name: openvino\n",
      "Version: 2023.3.0\n",
      "Summary: OpenVINO(TM) Runtime\n",
      "Home-page: https://docs.openvino.ai/2023.0/index.html\n",
      "Author: Intel(R) Corporation\n",
      "Author-email: openvino@intel.com\n",
      "License: OSI Approved :: Apache Software License\n",
      "Location: /home/mehrdad/pysdk/lib/python3.10/site-packages\n",
      "Requires: numpy, openvino-telemetry\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show degirum_tools || pip install degirum_tools\n",
    "!pip show openvino || pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading PySDK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "target = \"@cloud\"\n",
    "cloud_zoo_url = \"https://cs.degirum.com/degirum/openvino_demos\"\n",
    "cloud_token = degirum_tools.get_token()\n",
    "\n",
    "zoo = dg.connect(target, cloud_zoo_url, cloud_token)\n",
    "model = zoo.load_model(\"segmenter--512x512_float_openvino_cpu_1\")\n",
    "model.input_image_format = \"RAW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing preprocessing, inference support functions, and visualization functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now we will define utility functions for preprocessing, inferencing, and visualizing the results.\n",
    "\n",
    "### Preprocessing\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Inference input is tensor with shape `[1, 3, H, W]` in `B, C, H, W` format, where:\n",
    "\n",
    "* `B` - batch size (in our case 1, as we are just adding 1 with unsqueeze)\n",
    "* `C` - image channels (in our case RGB - 3)\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "Splitting to batches is done inside inference, so we don't need to split the image in preprocessing.\n",
    "\n",
    "Model expects images in RGB channels format, scaled to [0, 1] range and normalized with given mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization parameters\n",
    "normalization = {\"mean\": [0.5, 0.5, 0.5], \"std\": [0.5, 0.5, 0.5]}\n",
    "# number of model classes\n",
    "num_classes = 150\n",
    "# window size\n",
    "window_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import math\n",
    "\n",
    "def resize(im, smaller_size):\n",
    "    \"\"\"\n",
    "    Resize image bilinearly to make shorter side at least as long as the dimension provided.\n",
    "    \"\"\"\n",
    "    h, w = im.shape[2:]\n",
    "    if h < w:\n",
    "        ratio = w / h\n",
    "        h_res, w_res = smaller_size, ratio * smaller_size\n",
    "    else:\n",
    "        ratio = h / w\n",
    "        h_res, w_res = ratio * smaller_size, smaller_size\n",
    "    if min(h, w) < smaller_size:\n",
    "        im_res = cv2.resize(im, (int(h_res), int(w_res)), interpolation=cv2.INTER_LINEAR)\n",
    "    else:\n",
    "        im_res = im\n",
    "    return im_res\n",
    "\n",
    "def preprocess(im: Image, normalization: dict, window_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess image: scale, normalize, unsqueeze, and resize\n",
    "\n",
    "    :param im: input image\n",
    "    :param normalization: dictionary containing normalization data\n",
    "    :return:\n",
    "            im: processed (scaled and normalized) image\n",
    "    \"\"\"\n",
    "    # change PIL image to NumPy array and scale to [0, 1]\n",
    "    im = np.asarray(im, dtype=np.float32) / 255.0\n",
    "    # normalize by given mean and standard deviation\n",
    "    im -= np.array(normalization[\"mean\"])[None, None, :]\n",
    "    im /= np.array(normalization[\"std\"])[None, None, :]\n",
    "    # HWC -> CHW\n",
    "    im = np.transpose(im, (2, 0, 1))\n",
    "    # change dim from [C, H, W] to [1, C, H, W]\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    # resize image to window size by shorter dimension\n",
    "    im = resize(im, window_size)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Support\n",
    "The functions below implement a sliding window methodology for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(im, flip, window_size, window_stride):\n",
    "    \"\"\"\n",
    "    Create a batch of patches from the input image, based on provided window\n",
    "    size and stride.\n",
    "    \"\"\"\n",
    "    B, C, H, W = im.shape\n",
    "    ws = window_size\n",
    "\n",
    "    windows = {\"crop\": [], \"anchors\": []}\n",
    "    h_anchors = np.arange(0, H, window_stride)\n",
    "    w_anchors = np.arange(0, W, window_stride)\n",
    "    h_anchors = [h for h in h_anchors if h < H - ws] + [H - ws]\n",
    "    w_anchors = [w for w in w_anchors if w < W - ws] + [W - ws]\n",
    "    for ha in h_anchors:\n",
    "        for wa in w_anchors:\n",
    "            window = im[:, :, ha : ha + ws, wa : wa + ws]\n",
    "            windows[\"crop\"].append(window)\n",
    "            windows[\"anchors\"].append((ha, wa))\n",
    "    windows[\"flip\"] = flip\n",
    "    windows[\"shape\"] = (H, W)\n",
    "    return windows\n",
    "\n",
    "def bilinear_interpolation(original_img, new_shape):\n",
    "\t\"\"\"\n",
    "\tBilinear interpolation for masks in CHW format.\n",
    "\t\"\"\"\n",
    "\t#get dimensions of original image\n",
    "\toriginal_img = np.transpose(original_img, (1, 2, 0))\n",
    "\told_h, old_w, c = original_img.shape\n",
    "\tnew_h, new_w = new_shape\n",
    "\t#create an array of the desired shape. \n",
    "\t#We will fill-in the values later.\n",
    "\tresized = np.zeros((new_h, new_w, c), dtype=original_img.dtype)\n",
    "\t#Calculate horizontal and vertical scaling factor\n",
    "\tw_scale_factor = old_w / new_w\n",
    "\th_scale_factor = old_h / new_h\n",
    "\tfor i in range(new_h):\n",
    "\t\tfor j in range(new_w):\n",
    "\t\t\t#map the coordinates back to the original image\n",
    "\t\t\tx = i * h_scale_factor\n",
    "\t\t\ty = j * w_scale_factor\n",
    "\t\t\t#calculate the coordinate values for 4 surrounding pixels.\n",
    "\t\t\tx_floor = math.floor(x)\n",
    "\t\t\tx_ceil = min( old_h - 1, math.ceil(x))\n",
    "\t\t\ty_floor = math.floor(y)\n",
    "\t\t\ty_ceil = min(old_w - 1, math.ceil(y))\n",
    "\n",
    "\t\t\tif (x_ceil == x_floor) and (y_ceil == y_floor):\n",
    "\t\t\t\tq = original_img[int(x), int(y), :]\n",
    "\t\t\telif (x_ceil == x_floor):\n",
    "\t\t\t\tq1 = original_img[int(x), int(y_floor), :]\n",
    "\t\t\t\tq2 = original_img[int(x), int(y_ceil), :]\n",
    "\t\t\t\tq = q1 * (y_ceil - y) + q2 * (y - y_floor)\n",
    "\t\t\telif (y_ceil == y_floor):\n",
    "\t\t\t\tq1 = original_img[int(x_floor), int(y), :]\n",
    "\t\t\t\tq2 = original_img[int(x_ceil), int(y), :]\n",
    "\t\t\t\tq = (q1 * (x_ceil - x)) + (q2\t * (x - x_floor))\n",
    "\t\t\telse:\n",
    "\t\t\t\tv1 = original_img[x_floor, y_floor, :]\n",
    "\t\t\t\tv2 = original_img[x_ceil, y_floor, :]\n",
    "\t\t\t\tv3 = original_img[x_floor, y_ceil, :]\n",
    "\t\t\t\tv4 = original_img[x_ceil, y_ceil, :]\n",
    "\n",
    "\t\t\t\tq1 = v1 * (x_ceil - x) + v2 * (x - x_floor)\n",
    "\t\t\t\tq2 = v3 * (x_ceil - x) + v4 * (x - x_floor)\n",
    "\t\t\t\tq = q1 * (y_ceil - y) + q2 * (y - y_floor)\n",
    "\n",
    "\t\t\tresized[i,j,:] = q\n",
    "\tresized = np.transpose(resized, (2, 0, 1))\n",
    "\treturn resized\n",
    "\n",
    "def merge_windows(windows, window_size, ori_shape):\n",
    "    \"\"\"\n",
    "    Merge all inference result patches to create complete segmentation\n",
    "    mask output of the model.\n",
    "    \"\"\"\n",
    "    ws = window_size\n",
    "    im_windows = windows[\"seg_maps\"]\n",
    "    anchors = windows[\"anchors\"]\n",
    "    C = im_windows[0].shape[0]\n",
    "    H, W = windows[\"shape\"]\n",
    "    flip = windows[\"flip\"]\n",
    "\n",
    "    logit = np.zeros((C, H, W), dtype=np.float32)\n",
    "    count = np.zeros((1, H, W), dtype=np.float32)\n",
    "    for window, (ha, wa) in zip(im_windows, anchors):\n",
    "        logit[:, ha : ha + ws, wa : wa + ws] += window\n",
    "        count[:, ha : ha + ws, wa : wa + ws] += 1\n",
    "    logit = logit / count\n",
    "    logit = bilinear_interpolation(\n",
    "        logit,\n",
    "        ori_shape\n",
    "    )\n",
    "    if flip:\n",
    "        logit = np.flip(logit, axis=2)\n",
    "    result = softmax(logit, axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Function\n",
    "\n",
    "The following code implements the main inference function, utilizing the inference support functions defined above to split the input image into patches, run the model inference on each path, and merge together all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model,\n",
    "    num_classes,\n",
    "    ims,\n",
    "    ims_metas,\n",
    "    ori_shape,\n",
    "    window_size,\n",
    "    window_stride,\n",
    "    batch_size,\n",
    "):\n",
    "    C = num_classes\n",
    "    seg_map = np.zeros((C, ori_shape[0], ori_shape[1]))\n",
    "    for im, im_metas in zip(ims, ims_metas):\n",
    "        flip = im_metas[\"flip\"]\n",
    "        windows = sliding_window(im, flip, window_size, window_stride)\n",
    "        crops = np.stack(windows.pop(\"crop\"))[:, 0]\n",
    "        B = len(crops)\n",
    "        WB = batch_size\n",
    "        seg_maps = np.zeros((B, C, window_size, window_size))\n",
    "        for i in range(0, B, WB):\n",
    "            seg_maps[i : i + WB] = model(np.transpose(crops[i : i + WB], (0,2,3,1)).tobytes()).results[0]['data']\n",
    "        windows[\"seg_maps\"] = seg_maps\n",
    "        im_seg_map = merge_windows(windows, window_size, ori_shape)\n",
    "        seg_map += im_seg_map\n",
    "    seg_map /= len(ims)\n",
    "    return seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Inference output contains labels assigned to each pixel, so the output in our case is `[150, H, W]` in `CL, H, W` format where:\n",
    "\n",
    "* `CL` - number of classes for labels (in our case 150)\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "Since we want to visualize this output, we reduce dimensions to `[1, H, W]` where we keep only class with the highest value as that is the predicted label.\n",
    "We then combine original image with colors corresponding to the inferred labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "IGNORE_LABEL = 255\n",
    "ADE20K_CATS_PATH = \"ade20k.yml\"\n",
    "\n",
    "def seg_to_rgb(seg, colors):\n",
    "    im = np.zeros((seg.shape[0], seg.shape[1], seg.shape[2], 3), dtype=np.float32)\n",
    "    cls = np.unique(seg)\n",
    "    for cl in cls:\n",
    "        color = colors[int(cl)]\n",
    "        if len(color.shape) > 1:\n",
    "            color = color[0]\n",
    "        im[seg == cl] = color\n",
    "    return im\n",
    "\n",
    "def dataset_cat_description(path, cmap=None):\n",
    "    desc = yaml.load(open(path, \"r\"), Loader=yaml.FullLoader)\n",
    "    colors = {}\n",
    "    names = []\n",
    "    for i, cat in enumerate(desc):\n",
    "        names.append(cat[\"name\"])\n",
    "        if \"color\" in cat:\n",
    "            colors[cat[\"id\"]] = np.array(cat[\"color\"], dtype=np.float32) / 255.0\n",
    "        else:\n",
    "            colors[cat[\"id\"]] = np.array(cmap[cat[\"id\"]], dtype=np.float32)\n",
    "    colors[IGNORE_LABEL] = np.array([0.0, 0.0, 0.0], dtype=np.float32)\n",
    "    return names, colors\n",
    "\n",
    "def apply_segmentation_mask(pil_im: Image, results: np.ndarray) -> Image:\n",
    "    \"\"\"\n",
    "    Combine segmentation masks with the image\n",
    "\n",
    "    :param pil_im: original input image\n",
    "    :param results: tensor containing segmentation masks for each pixel\n",
    "    :return:\n",
    "            pil_blend: image with colored segmentation masks overlay\n",
    "    \"\"\"\n",
    "    cat_names, cat_colors = dataset_cat_description(ADE20K_CATS_PATH)\n",
    "\n",
    "    # 3D array, where each pixel has values for all classes, take index of max as label\n",
    "    seg_map = np.argmax(results, axis=0, keepdims=True)\n",
    "    # transform label id to colors\n",
    "    seg_rgb = seg_to_rgb(seg_map, cat_colors)\n",
    "    seg_rgb = (255 * seg_rgb).astype(np.uint8)\n",
    "    pil_seg = Image.fromarray(seg_rgb[0])\n",
    "\n",
    "    # overlay segmentation mask over original image\n",
    "    pil_blend = Image.blend(pil_im, pil_seg, 0.5).convert(\"RGB\")\n",
    "\n",
    "    return pil_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of inference of PySDK model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now that we have everything ready, we can perform segmentation on example image `coco_hollywood.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads an image from `path` and returns it as BGR numpy array. `path`\n",
    "    should point to an image file, either a local filename or a url. The image is\n",
    "    not stored to the filesystem. Use the `download_file` function to download and\n",
    "    store an image.\n",
    "\n",
    "    :param path: Local path name or URL to image.\n",
    "    :return: image as BGR numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    if path.startswith(\"http\"):\n",
    "        # Set User-Agent to Mozilla because some websites block\n",
    "        # requests with User-Agent Python\n",
    "        response = requests.get(path, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        array = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(array, -1)  # Loads the image as BGR\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image with OpenCV\n",
    "image = load_image(\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_hollywood.jpg\")\n",
    "# load_image reads the image in BGR format, [:,:,::-1] reshape transfroms it to RGB\n",
    "pil_image = Image.fromarray(image[:,:,::-1])\n",
    "\n",
    "# preprocess image with normalization params loaded in previous steps\n",
    "image = preprocess(pil_image, normalization, window_size)\n",
    "\n",
    "# inference function needs some meta parameters, where we specify that we don't flip images in inference mode\n",
    "im_meta = dict(flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference with same function as in case of PyTorch model from repository\n",
    "results = inference(model=model,\n",
    "                    num_classes=num_classes,\n",
    "                    ims=[image],\n",
    "                    ims_metas=[im_meta],\n",
    "                    ori_shape=image.shape[2:4],\n",
    "                    window_size=window_size,\n",
    "                    window_stride=window_size,\n",
    "                    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "converted_blend = apply_segmentation_mask(pil_image, results)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "converted_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/61357777/223854308-d1ac4a39-cc0c-4618-9e4f-d9d4d8b991e8.jpg",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image Segmentation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
