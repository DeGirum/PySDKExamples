{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Recognition with Degirum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human action recognition finds actions over time in a video. The list of actions in this notebook is extensive (400 in total) and covers Person Actions, (for example, drawing, drinking, laughing), Person-Person Actions (for example, hugging, shaking hands), and Person-Object Actions (for example, opening present, mowing the lawn, playing \"instrument\"). You could find several parent-child groupings on the list of labels, such as braiding hair and brushing hair, salsa dancing, robot dancing, or playing violin and playing guitar. For more information about the labels and the dataset, see the \"The Kinetics Human Action Video Dataset\" research paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates live human action recognition with OpenVINO, using the [Action Recognition Models](https://docs.openvino.ai/2024/omz_models_group_intel.html#action-recognition-models) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), specifically the Encoder and Decoder from [action-recognition-0001](https://docs.openvino.ai/2024/omz_models_model_action_recognition_0001.html). Both models create a sequence to sequence (\"seq2seq\")1 system to identify the human activities for Kinetics-400 dataset. The models use the Video Transformer approach with ResNet34 encoder2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify where you want to run your inferences, model_zoo_url, model name for action recognition and video source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_name: name of the model for running AI inference\n",
    "# image_source: image source for inference\n",
    "#     path to image file\n",
    "#     URL of image\n",
    "#     PIL image object\n",
    "#     numpy array\n",
    "\n",
    "hw_location = \"@cloud\"\n",
    "model_zoo_url = \"https://cs.degirum.com/degirum/openvino_demos\"\n",
    "encoder_model_name = \"encoder_action_recognition--224x224_float_openvino_cpu_1\"\n",
    "decoder_model_name = \"decoder_action_recognition--224x224_float_openvino_cpu_1\"\n",
    "video_source = \"https://archive.org/serve/ISSVideoResourceLifeOnStation720p/ISS%20Video%20Resource_LifeOnStation_720p.mp4\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython import display\n",
    "from preprocessor_action_rec_encoder import ActionRecEncoderPreprocessor\n",
    "from postprocessor_action_rec_decoder import ActionRecDecoderPostprocessor\n",
    "from image_overlay import display_text_fnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Action Recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_rec_zoo = dg.connect(hw_location, model_zoo_url, degirum_tools.get_token())\n",
    "encoder_model = action_rec_zoo.load_model(encoder_model_name)\n",
    "decoder_model = action_rec_zoo.load_model(decoder_model_name, custom_postprocessor = ActionRecDecoderPostprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_w, dec_h = decoder_model.model_info.InputW[0],decoder_model.model_info.InputC[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Action Recognition on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 100  # Maximum number of frames to read from video, set to 0 for all frames.\n",
    "sample_duration = 16\n",
    "skip_first_frames = 600\n",
    "size = encoder_model.model_info.InputC[0]\n",
    "use_popup=False\n",
    "with degirum_tools.open_video_stream(video_source) as video_stream:\n",
    "    w, h, fps = degirum_tools.get_video_stream_properties(video_stream)\n",
    "    fps = 30\n",
    "    if num_frames == 0:\n",
    "        total_frames = video_stream.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    else:\n",
    "        total_frames = num_frames\n",
    "    encoder_output = []\n",
    "    decoder_output = []\n",
    "    counter = 0\n",
    "    frames = []\n",
    "    text_template = \"{label},{conf:.2f}%\"\n",
    "    progress = degirum_tools.Progress(total_frames)\n",
    "    if use_popup:\n",
    "        title = \"Press ESC to Exit\"\n",
    "        cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)  \n",
    "        \n",
    "    for i, frame in enumerate(degirum_tools.video_source(video_stream)):\n",
    "        if i < skip_first_frames:\n",
    "            continue\n",
    "        if i == skip_first_frames+total_frames:\n",
    "            break\n",
    "        counter = counter + 1\n",
    "        \n",
    "        scale = 1280 / max(frame.shape)\n",
    "        # Adaptative resize for visualization.\n",
    "        if scale < 1:\n",
    "            frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        if counter % 2 == 0:\n",
    "            preprocessed = ActionRecEncoderPreprocessor(size).preprocess_frame_for_encoder(frame)\n",
    "            encoder_output.append(encoder_model(preprocessed).results[0][\"data\"])\n",
    "            if len(encoder_output) == sample_duration:\n",
    "                decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "                # Organize input shape vector to the Decoder (shape: [1x16x512]]\n",
    "                decoder_input = decoder_input.transpose((2, 3, 0, 1))\n",
    "                decoder_input = decoder_input.reshape((1, dec_w, dec_h))\n",
    "                decoder_input = decoder_input.astype(np.float32)\n",
    "                result_de = decoder_model(decoder_input) \n",
    "                decoder_output = result_de.results\n",
    "                encoder_output = []     \n",
    "\n",
    "        # Visualize the results.\n",
    "        if decoder_output:\n",
    "            for idx in range(len(result_de.results)):\n",
    "                display_text = text_template.format(\n",
    "                    label=result_de.results[idx]['label'],\n",
    "                    conf=result_de.results[idx]['score']*100,\n",
    "                )\n",
    "                display_text_fnc(size, frame, display_text, idx)\n",
    "\n",
    "        # Use this workaround if you experience flickering.\n",
    "        if use_popup:\n",
    "            cv2.imshow(title, frame)\n",
    "            key = cv2.waitKey(1)\n",
    "            # escape = 27\n",
    "            if key == 27:\n",
    "                break\n",
    "        else:\n",
    "            # Encode numpy array to jpg.\n",
    "            _, encoded_img = cv2.imencode(\".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "            # Create an IPython image.\n",
    "            i = display.Image(data=encoded_img)\n",
    "            # Display the image in this notebook.\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(i)           \n",
    "        progress.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
