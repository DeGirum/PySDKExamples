{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f257328f",
   "metadata": {},
   "source": [
    "## Example script illustrating sound classification on audio stream\n",
    "This notebook is an example how to use DeGirum PySDK to do sound classification AI inference of an audio stream from local microphone.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "**pyaudio package with portaudio is required to run this sample.**\n",
    "\n",
    "**Access to microphone is required to run this sample.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227c649-6c23-41d1-a6df-4247f4a6a480",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef133f4-8197-4de5-a44e-c76dbbd39a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "cloud_token = mytools.get_token() # get cloud API access token from env.ini file\n",
    "cloud_zoo_url = mytools.get_cloud_zoo_url() # get cloud zoo URL from env.ini file\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "# 1. Inference on the DeGirum Cloud Platform\n",
    "zoo = dg.connect_model_zoo(\"dgcps://cs.degirum.com\" + cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 2. Inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN\n",
    "# zoo = dg.connect_model_zoo((mytools.get_ai_server_hostname(), \"https://cs.degirum.com\" + cloud_zoo_url), cloud_token)\n",
    "\n",
    "# 3. Inference on DeGirum ORCA accelerator installed on your computer\n",
    "# zoo = dg.connect_model_zoo(\"https://cs.degirum.com\" + cloud_zoo_url, cloud_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86162da-d4bc-42d6-b839-b10025306796",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a1753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd775c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load YAMNET sound classification model for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "model = zoo.load_model(\"mobilenet_v1_yamnet_sound_cls--96x64_quant_n2x_orca_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db989e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abort = False # stream abort flag\n",
    "N = 5 # inference results history depth\n",
    "history = [] # list of N consecutive inference results\n",
    "\n",
    "sampling_rate_hz = model.model_info.InputSamplingRate[0]\n",
    "read_buffer_size = model.model_info.InputWaveformSize[0] // 2 # two read buffers in waveform for half-length overlapping\n",
    "\n",
    "# Acquire model input stream object\n",
    "with mytools.open_audio_stream(sampling_rate_hz, read_buffer_size) as stream:\n",
    "    #\n",
    "    # AI prediction loop.\n",
    "    # emit keyboard typing sound to stop\n",
    "    #\n",
    "    for res in model.predict_batch(mytools.audio_overlapped_source(stream, lambda: abort)):\n",
    "        # clear Jupyter output cell\n",
    "        clear_output(wait = True) \n",
    "        \n",
    "        # add top inference result to history\n",
    "        history.insert(0, f\"{res.results[0]['label']}: {res.results[0]['score']}\" )\n",
    "    \n",
    "        # keep only N last elements in history\n",
    "        if len(history) > N:\n",
    "            history.pop()\n",
    "    \n",
    "        # print history\n",
    "        for m in history:\n",
    "            print(m)\n",
    "        \n",
    "        # check for stop condition\n",
    "        if res.results[0]['label'] == \"Typing\":\n",
    "            abort = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63eee6-e66d-4a23-bce6-1d4bcf41cce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
