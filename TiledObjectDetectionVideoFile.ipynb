{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Tiled object detection from a video file with optional motion detection\n",
    "This notebook is an example how to use DeGirum PySDK to do tiled object detection of a video stream from a video file.\n",
    "Each video frame is divided by tiles with some overlap, each tile of the AI model input size (to avoid resizing).\n",
    "Object detection is performed for each tile, then results from different tiles are combined.\n",
    "The annotated video is saved into new file with `_tiled_annotated` suffix.\n",
    "If motion detection mode is turned on, then areas with motion are detected for each frame, and only tiles, where\n",
    "motion is detected, are processed.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify video file name, model name, and other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_filename = \"./images/TrafficHD.mp4\" # video file to process\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca_1\" # model to use\n",
    "min_overlap_precent = [20,20] # minimum tile overlap (in percent of tile dimensions)\n",
    "classes = [\"car\"] # list of classes to show\n",
    "do_motion_detection = True # enable motion detection: do inference only in tiles, where motion is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533830-1888-4c56-8883-1d53bb81b1e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "cloud_token = mytools.get_token() # get cloud API access token from env.ini file\n",
    "cloud_zoo_url = mytools.get_cloud_zoo_url() # get cloud zoo URL from env.ini file\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "# 1. Inference on the DeGirum Cloud Platform\n",
    "zoo = dg.connect_model_zoo(\"dgcps://cs.degirum.com\" + cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 2. Inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN\n",
    "# zoo = dg.connect_model_zoo((mytools.get_ai_server_hostname(), \"https://cs.degirum.com\" + cloud_zoo_url), cloud_token)\n",
    "\n",
    "# 3. Inference on DeGirum ORCA accelerator installed on your computer\n",
    "# zoo = dg.connect_model_zoo(\"https://cs.degirum.com\" + cloud_zoo_url, cloud_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1b821-e18e-403b-8147-9f95fc6cfa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512335c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2, math, threading, queue, numpy as np\n",
    "from pathlib import Path\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f3bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load object detection model\n",
    "model = zoo.load_model(model_name)\n",
    "\n",
    "# set model parameters\n",
    "model.image_backend = 'opencv' # select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "model.input_numpy_colorspace = 'BGR'\n",
    "model.overlay_show_probabilities = False\n",
    "model.overlay_show_labels = False\n",
    "model.overlay_line_width = 1\n",
    "model.overlay_alpha = 1\n",
    "model._model_parameters.InputImgFmt = ['JPEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c581e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect areas with motion on given image in respect to base image.\n",
    "# Returns a tuple of motion image and updated base image.\n",
    "# Motion image is black image with white pixels where motion is detected.\n",
    "def detectMotion(base_img, img):\n",
    "\n",
    "    cur_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cur_img = cv2.GaussianBlur(src=cur_img, ksize=(5,5), sigmaX=0)\n",
    "    \n",
    "    if base_img is None:\n",
    "        base_img = cur_img\n",
    "        return None, base_img\n",
    "        \n",
    "    diff = cv2.absdiff(base_img, cur_img)    \n",
    "    base_img = cur_img\n",
    "    \n",
    "    _, thresh = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.dilate(thresh, None)\n",
    "    \n",
    "    return thresh, base_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3ecae-3162-4e6d-9157-6010a6db4964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define source of tile frames to be used in batch predict\n",
    "def source(stream, model, min_overlap_precent, progress):\n",
    "    \n",
    "    tile_w, tile_h = model.model_info.InputW[0], model.model_info.InputH[0]\n",
    "    image_w, image_h = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # function to calculate optimal overlap (0..1) and number of tiles\n",
    "    def calc_overlap(tile_dim, image_dim, min_overlap_precent):\n",
    "        tiles_less_one = math.ceil((image_dim - tile_dim) / (tile_dim * (1. - 0.01 * min_overlap_precent)))\n",
    "        return 1. - (image_dim - tile_dim) / (tiles_less_one * tile_dim), tiles_less_one + 1\n",
    "    \n",
    "    x_overlap, x_tiles = calc_overlap(tile_w, image_w, min_overlap_precent[0])\n",
    "    y_overlap, y_tiles = calc_overlap(tile_h, image_h, min_overlap_precent[1])\n",
    "    print(f\"Full frame: {image_w}x{image_h}, tile: {tile_w}x{tile_h}, overlap: {round(x_overlap*100)}x{round(y_overlap*100)}%, tiles: {x_tiles}x{y_tiles}={x_tiles*y_tiles}\")\n",
    "    \n",
    "    base_img = None # base imnage for motion detection\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        progress.step()\n",
    "        \n",
    "        # loop over tiles\n",
    "        first_tile = True\n",
    "        \n",
    "        if do_motion_detection:\n",
    "            motion_img, base_img = detectMotion(base_img, frame)\n",
    "            if motion_img is None:\n",
    "                continue\n",
    "        \n",
    "        for xi in range(x_tiles):\n",
    "            for yi in range(y_tiles):\n",
    "                x, y = math.floor(xi * tile_w * (1 - x_overlap)), math.floor(yi * tile_h * (1 - y_overlap))\n",
    "                \n",
    "                if do_motion_detection:\n",
    "                    if cv2.countNonZero(motion_img[y : y + tile_h, x : x + tile_w]) == 0:\n",
    "                        continue\n",
    "                \n",
    "                tile = frame[y : y + tile_h, x : x + tile_w]\n",
    "                info = { \"first_tile\": first_tile, \"frame\": frame, \"topleft\": (x, y), \"tilesize\": (tile_w, tile_h) }\n",
    "                first_tile = False\n",
    "                yield (tile, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253d6f5-b2b7-46b0-a6ff-0f5c5f3f8dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine results of multiple tiles\n",
    "def combine(combined_result, new_result, iou_threshold=0.5):\n",
    "    \n",
    "    # filter classes\n",
    "    new_result._inference_results = [ res for res in new_result._inference_results if res[\"label\"] in classes ]\n",
    "    \n",
    "    # convert bbox coordinates to full image\n",
    "    topleft = new_result.info[\"topleft\"]\n",
    "    for r in new_result._inference_results:\n",
    "        r[\"bbox\"] = list(np.array(r[\"bbox\"]) + (topleft + topleft))\n",
    "    \n",
    "    if not combined_result:\n",
    "        # first tile result: just store\n",
    "        combined_result = new_result\n",
    "        combined_result._input_image = new_result.info[\"frame\"]\n",
    "    else:\n",
    "        # consecutive tile result: merge bboxes\n",
    "        for new_res in new_result._inference_results:\n",
    "            for res in combined_result._inference_results:\n",
    "                bboxes = np.array([new_res[\"bbox\"], res[\"bbox\"]])\n",
    "                areas = mytools.area(bboxes)\n",
    "                intersection = mytools.intersection(bboxes[0], bboxes[1])\n",
    "                if intersection / min(areas) >= iou_threshold:                   \n",
    "                    # take biggest box\n",
    "                    if areas[0] > areas[1]:\n",
    "                        res[\"bbox\"] = new_res[\"bbox\"]\n",
    "                    break\n",
    "            else: # this clause is executed when `for` loop has no breaks\n",
    "                # this box is genuine: just add it as is\n",
    "                combined_result._inference_results.append(new_res)\n",
    "    \n",
    "    return combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906309-0ea3-458f-a1c4-282b2de56a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_path = Path(input_filename)\n",
    "ann_path = orig_path.with_name(orig_path.stem + \"_tiled_annotated\" + orig_path.suffix)\n",
    "abort = False\n",
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "with mytools.Display(\"Tiled Detectoon\", not do_motion_detection) as display, \\\n",
    "     mytools.open_video_stream(input_filename) as stream, \\\n",
    "     mytools.open_video_writer(str(ann_path), stream.get(cv2.CAP_PROP_FRAME_WIDTH), stream.get(cv2.CAP_PROP_FRAME_HEIGHT)) as writer:     \n",
    "         \n",
    "    # do image processing in separate thread to improve performance\n",
    "    result_queue = queue.Queue()\n",
    "    def worker():\n",
    "        global abort\n",
    "        try:\n",
    "            while True:\n",
    "                result = result_queue.get()\n",
    "                if result is None:\n",
    "                    break;\n",
    "                img = result.image_overlay\n",
    "                writer.write(img)\n",
    "                \n",
    "                if do_motion_detection:\n",
    "                    mytools.Display.put_text(img, \n",
    "                        f\"Motion tiles: {result.info['tiles_cnt']:2d}\", (0, 0), (0, 0, 0), (255, 255, 255))\n",
    "                display.show(img)\n",
    "        except KeyboardInterrupt:\n",
    "            abort = True\n",
    "                \n",
    "    threading.Thread(target=worker).start()\n",
    "    \n",
    "    progress = mytools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "    combined_result = None\n",
    "    tiles_cnt = 0\n",
    "    \n",
    "    # inference loop\n",
    "    for res in model.predict_batch(source(stream, model, min_overlap_precent, progress)):\n",
    "        if res.info[\"first_tile\"] and combined_result: # new frame started\n",
    "            combined_result.info[\"tiles_cnt\"] = tiles_cnt\n",
    "            result_queue.put(combined_result)\n",
    "            combined_result = None\n",
    "            tiles_cnt = 0\n",
    "\n",
    "        combined_result = combine(combined_result, res)\n",
    "        tiles_cnt += 1\n",
    "        if abort:\n",
    "            break\n",
    "        \n",
    "    result_queue.put(None) # to stop worker thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211246e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display result\n",
    "IPython.display.Video(ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original video\n",
    "IPython.display.Video(orig_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
