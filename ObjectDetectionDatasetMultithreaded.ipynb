{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Parallel AI Inference on image dataset retrieved from a cloud\n",
    "This notebook is an example how to use DeGirum PySDK to do multi-threaded AI inference of an image dataset.\n",
    "Image dataset is retrieved from the cloud using `fiftyone` API.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. [DeGirum Cloud Platform](https://cs.degirum.com),\n",
    "1. DeGirum-hosted AI server node shared via Peer-to-Peer VPN,\n",
    "1. AI server node hosted by you in your local network,\n",
    "1. AI server running on your local machine,\n",
    "1. DeGirum ORCA accelerator directly installed on your local machine.\n",
    "\n",
    "To try different options, you just need to change the `inference_option` in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specify where do you want to run your inferences and dataset parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_option = 1  # <<< change it according to your needs selecting from the list in the header comment\n",
    "\n",
    "# specify the name of desired dataset to retrieve;\n",
    "# see https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html\n",
    "foz_dataset_name = \"coco-2017\"\n",
    "\n",
    "# specify list of class labels to retrieve; None for all classes\n",
    "foz_classes = None # [\"car\", \"cup\", \"person\"]\n",
    "\n",
    "# specify which splits to download (\"train\", \"validation\", \"test\")\n",
    "foz_splits = \"validation\"\n",
    "\n",
    "# specify # of samples in dataset to retrieve\n",
    "samples_num = 1000\n",
    "\n",
    "# specify the model to be used for inference\n",
    "model_name = \"yolo_v5s_coco--576x576_quant_n2x_orca_1\"\n",
    "\n",
    "# specify number of parallel threads to run inference\n",
    "nthreads = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4549b06-567f-4cdb-8b82-bb4ef5858f2f",
   "metadata": {},
   "source": [
    "### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e512335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg # import DeGirum PySDK\n",
    "import mytools\n",
    "import threading, queue, sys\n",
    "fo = mytools.import_fiftyone() # import 'fiftyone' package for dataset management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "dataset = fo.zoo.load_zoo_dataset(\n",
    "    foz_dataset_name,\n",
    "    dataset_dir=\"./my-dataset\", \n",
    "    classes=foz_classes,\n",
    "    split=foz_splits,\n",
    "    max_samples=samples_num,\n",
    "    shuffle=True,\n",
    "    drop_existing_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to model zoo according to selected inference option\n",
    "zoo = mytools.connect_model_zoo(inference_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define worker thread function:\n",
    "# it takes frames from `inqueue`, performs ML inference, and puts results into `outqueue`\n",
    "def worker(inqueue, outqueue):\n",
    "        \n",
    "    # load model\n",
    "    model = zoo.load_model(model_name)\n",
    "    model.output_confidence_threshold = 0.1\n",
    "    myframes = []\n",
    "\n",
    "    # define frame source generator function:\n",
    "    # it gets frames from input queue and yields frame filenames one at a time\n",
    "    def source():\n",
    "        while True:\n",
    "            frame = inqueue.get()\n",
    "            if frame is None:\n",
    "                # poison pill received: return poison pill back to queue to stop other threads\n",
    "                inqueue.put(None)\n",
    "                break\n",
    "            myframes.append(frame)\n",
    "            yield frame.filepath\n",
    "\n",
    "    # do batch prediction on frame source\n",
    "    for res in model.predict_batch(source()):\n",
    "        # put a tuple of inference result and input frame into output queue\n",
    "        outqueue.put((res, myframes.pop(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338279d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create inference task and result queues\n",
    "task_queue = queue.Queue()\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# fill task queue with data from dataset\n",
    "nframes = dataset.count()\n",
    "print(f\"Queueing {nframes} frames...\")\n",
    "for frame in dataset:\n",
    "    task_queue.put(frame)\n",
    "\n",
    "task_queue.put(None) # poison pill is last\n",
    "\n",
    "# run ML inference in threads\n",
    "print(f\"Running inference...\")\n",
    "tmr = mytools.Timer()\n",
    "threads = []\n",
    "for n in range(nthreads): # start worker threads\n",
    "    t = threading.Thread(target=worker, args=(task_queue, result_queue))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads: # wait for completion\n",
    "    t.join()\n",
    "\n",
    "elapsed_s = tmr()\n",
    "print(f\"Inference done: {nframes} frames in {elapsed_s:.1f} s = {nframes/elapsed_s:.1f} FPS\")\n",
    "\n",
    "# process result queue: add inference results to dataset\n",
    "print(f\"Processing results...\")\n",
    "for n in range(nframes):\n",
    "    res, frame = result_queue.get()\n",
    "    w, h = res.image.size\n",
    "    detections = []\n",
    "    for box in res.results:\n",
    "        # Convert to [top-left-x, top-left-y, width, height]\n",
    "        # in relative coordinates in [0, 1] x [0, 1]\n",
    "        x1, y1, x2, y2 = box[\"bbox\"]\n",
    "        rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
    "        detections.append(fo.Detection(label=box[\"label\"], bounding_box=rel_box, confidence=box[\"score\"]))\n",
    "    frame[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    frame.save()\n",
    "\n",
    "print(f\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation on predictions\n",
    "eval_result = dataset.evaluate_detections(\"predictions\", compute_mAP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some evaluation results\n",
    "print( f\"mAP = {eval_result.mAP():.2f}\\n\")\n",
    "eval_result.print_report(classes=foz_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee99560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
