{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Parallel AI Inference on image dataset retrieved from a cloud\n",
    "This notebook is an example how to use DeGirum PySDK to do multi-threaded AI inference of an image dataset.\n",
    "Image dataset is retrieved from the cloud using `fiftyone` API.\n",
    "\n",
    "#### This sample uses the following external packages, which need to be installed:\n",
    "1. **fiftyone**: `pip install fiftyone`\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify dataset parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the name of desired dataset to retrieve;\n",
    "# see https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html\n",
    "foz_dataset_name = \"coco-2017\"\n",
    "\n",
    "# specify list of class labels to retrieve; None for all classes\n",
    "foz_classes = None # [\"car\", \"cup\", \"person\"]\n",
    "\n",
    "# specify which splits to download (\"train\", \"validation\", \"test\")\n",
    "foz_splits = \"validation\"\n",
    "\n",
    "# specify # of samples in dataset to retrieve\n",
    "samples_num = 1000\n",
    "\n",
    "# specify the model to be used for inference\n",
    "model_name = \"yolo_v5s_coco--576x576_quant_n2x_orca_1\"\n",
    "\n",
    "# specify number of parallel threads to run inference\n",
    "nthreads = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec660ca-74aa-45df-be6e-acf29b1b08de",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349a091-9b1e-4bdd-bda1-01d1cc962df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "cloud_token = mytools.get_token() # get cloud API access token from env.ini file\n",
    "cloud_zoo_url = mytools.get_cloud_zoo_url() # get cloud zoo URL from env.ini file\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "# 1. Inference on the DeGirum Cloud Platform\n",
    "zoo = dg.connect(dg.CLOUD, cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 2. Inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN\n",
    "# zoo = dg.connect(mytools.get_ai_server_hostname(), cloud_zoo_url, cloud_token)\n",
    "\n",
    "# 3. Inference on DeGirum ORCA accelerator installed on your computer\n",
    "# zoo = dg.connect(dg.LOCAL, cloud_zoo_url, cloud_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5b5b3-fc5d-4001-9818-179f3dc205a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512335c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading, queue, sys\n",
    "\n",
    "fo = mytools.import_fiftyone() # import 'fiftyone' package for dataset management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d329b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download dataset\n",
    "dataset = fo.zoo.load_zoo_dataset(\n",
    "    foz_dataset_name,\n",
    "    dataset_dir=\"./my-dataset\", \n",
    "    classes=foz_classes,\n",
    "    split=foz_splits,\n",
    "    max_samples=samples_num,\n",
    "    shuffle=True,\n",
    "    drop_existing_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f3bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define worker thread function:\n",
    "# it takes frames from `inqueue`, performs ML inference, and puts results into `outqueue`\n",
    "def worker(inqueue, outqueue):\n",
    "        \n",
    "    # load model\n",
    "    model = zoo.load_model(model_name)\n",
    "    model.output_confidence_threshold = 0.1\n",
    "    myframes = []\n",
    "\n",
    "    # define frame source generator function:\n",
    "    # it gets frames from input queue and yields frame filenames one at a time\n",
    "    def source():\n",
    "        while True:\n",
    "            frame = inqueue.get()\n",
    "            if frame is None:\n",
    "                # poison pill received: return poison pill back to queue to stop other threads\n",
    "                inqueue.put(None)\n",
    "                break\n",
    "            myframes.append(frame)\n",
    "            yield frame.filepath\n",
    "\n",
    "    # do batch prediction on frame source\n",
    "    for res in model.predict_batch(source()):\n",
    "        # put a tuple of inference result and input frame into output queue\n",
    "        outqueue.put((res, myframes.pop(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338279d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create inference task and result queues\n",
    "task_queue = queue.Queue()\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# fill task queue with data from dataset\n",
    "nframes = dataset.count()\n",
    "print(f\"Queueing {nframes} frames...\")\n",
    "for frame in dataset:\n",
    "    task_queue.put(frame)\n",
    "\n",
    "task_queue.put(None) # poison pill is last\n",
    "\n",
    "# run ML inference in threads\n",
    "print(f\"Running inference...\")\n",
    "tmr = mytools.Timer()\n",
    "threads = []\n",
    "for n in range(nthreads): # start worker threads\n",
    "    t = threading.Thread(target=worker, args=(task_queue, result_queue))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads: # wait for completion\n",
    "    t.join()\n",
    "\n",
    "elapsed_s = tmr()\n",
    "print(f\"Inference done: {nframes} frames in {elapsed_s:.1f} s = {nframes/elapsed_s:.1f} FPS\")\n",
    "\n",
    "# process result queue: add inference results to dataset\n",
    "print(f\"Processing results...\")\n",
    "for n in range(nframes):\n",
    "    res, frame = result_queue.get()\n",
    "    w, h = res.image.size\n",
    "    detections = []\n",
    "    for box in res.results:\n",
    "        # Convert to [top-left-x, top-left-y, width, height]\n",
    "        # in relative coordinates in [0, 1] x [0, 1]\n",
    "        x1, y1, x2, y2 = box[\"bbox\"]\n",
    "        rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
    "        detections.append(fo.Detection(label=box[\"label\"], bounding_box=rel_box, confidence=box[\"score\"]))\n",
    "    frame[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    frame.save()\n",
    "\n",
    "print(f\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run evaluation on predictions\n",
    "eval_result = dataset.evaluate_detections(\"predictions\", compute_mAP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899c3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print some evaluation results\n",
    "print( f\"mAP = {eval_result.mAP():.2f}\\n\")\n",
    "eval_result.print_report(classes=foz_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee99560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
