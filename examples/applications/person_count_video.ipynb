{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "## Counting People in a Video Frame-by-Frame\n",
    "This notebook demonstrates the algorithm of counting people in a video frame, and the use of this algorithm to annotate a video.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on the DeGirum Cloud Platform;\n",
    "2. Run inference on a DeGirum AI Server deployed on the local host or on some computer in your LAN or VPN;\n",
    "3. Run inference on a DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option. \n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`.\n",
    "\n",
    "Note: Please specify a path to an input video source before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum and degirum-tools packages are installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify where you want to run your inferences, model zoo url, model name and video source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# person_model_zoo_url: url/path for person model zoo\n",
    "#     Use cloud_zoo_url for @cloud, @local, and AI server inference options.\n",
    "#     Use '' for an AI server serving models from a local folder.\n",
    "#     Use a path to a JSON file for a single model zoo in case of @local inference.\n",
    "# person_model_name: name of the model for person detection.\n",
    "# head_model_zoo_url: URL/path for the head model zoo.\n",
    "# head_model_name: name of the model for head detection.\n",
    "# face_model_zoo_url: URL/path for the face model zoo.\n",
    "# face_model_name: name of the model for face detection.\n",
    "# video_source: video source for inference\n",
    "#     camera index for local camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "# degirum_cloud_token: your token for accessing the DeGirum cloud platform\n",
    "hw_location = \"localhost\"\n",
    "person_model_zoo_url = \"https://cs.degirum.com/degirum/person_detection\"\n",
    "person_model_name = \"yolov8m_relu6_person--960x960_float_openvino_cpu_1\"\n",
    "head_model_zoo_url = \"https://cs.degirum.com/degirum/human_head_detection\"\n",
    "head_model_name = \"yolov8s_relu6_human_head--960x960_float_openvino_cpu_1\"\n",
    "face_model_zoo_url = \"https://cs.degirum.com/degirum/face_detection\"\n",
    "face_model_name = \"yolov8s_relu6_face--640x640_float_openvino_cpu_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify arguments for the annotation process (please specify the models, threshold value, and video source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection models to be used in the person counting algorithm;\n",
    "# can be one or any combination of the following - 'person', 'head', 'face'\n",
    "models = [\"person\", \"head\", \"face\"]\n",
    "\n",
    "# IoMA (Intersection over Minimum Area) threshold for matching bounding boxes\n",
    "threshold = 0.7\n",
    "\n",
    "# Paths to video source and output video\n",
    "video_source = \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/store_short.mp4\"\n",
    "output_video = \"temp/person_count_video.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to model zoos, load the corresponding detection models, and create the combined model to use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools as dgtools\n",
    "\n",
    "# Connect to model zoos and load models\n",
    "person_zoo = dg.connect(hw_location)  # , person_model_zoo_url, dgtools.get_token())\n",
    "head_zoo = dg.connect(hw_location)  # , head_model_zoo_url, dgtools.get_token())\n",
    "face_zoo = dg.connect(hw_location)  # , face_model_zoo_url, dgtools.get_token())\n",
    "\n",
    "# Set up models\n",
    "model = {}\n",
    "model[\"person\"] = person_zoo.load_model(\n",
    "    person_model_name,\n",
    "    overlay_show_labels=False,\n",
    "    overlay_line_width=2,\n",
    "    input_letterbox_fill_color=(114, 114, 114),\n",
    ")\n",
    "model[\"head\"] = head_zoo.load_model(\n",
    "    head_model_name,\n",
    "    overlay_show_labels=False,\n",
    "    overlay_line_width=2,\n",
    "    input_letterbox_fill_color=(114, 114, 114),\n",
    ")\n",
    "model[\"face\"] = face_zoo.load_model(\n",
    "    face_model_name,\n",
    "    overlay_show_labels=False,\n",
    "    overlay_line_width=2,\n",
    "    input_letterbox_fill_color=(114, 114, 114),\n",
    ")\n",
    "\n",
    "# Create combined model from individual models\n",
    "for i, model_name in enumerate(models):\n",
    "    if i == 0:\n",
    "        combined_model = model[model_name]\n",
    "    else:\n",
    "        combined_model = dgtools.CombiningCompoundModel(\n",
    "            combined_model, model[model_name]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### People Counting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ioma(box1, box2):\n",
    "    \"\"\"Calculate intersection over minimum area (IoMA) between two bounding boxes.\"\"\"\n",
    "    # Unpack the bounding box coordinates\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2\n",
    "\n",
    "    if b1_x1 > b2_x2 or b1_x2 < b2_x1 or b1_y1 > b2_y2 or b1_y2 < b2_y1:\n",
    "        return 0.0  # If the boxes don't overlap, return 0\n",
    "\n",
    "    # Compute intersection coordinates\n",
    "    intersection = [\n",
    "        max(b1_x1, b2_x1),\n",
    "        max(b1_y1, b2_y1),\n",
    "        min(b1_x2, b2_x2),\n",
    "        min(b1_y2, b2_y2),\n",
    "    ]\n",
    "\n",
    "    # Compute areas of the boxes and their intersection\n",
    "    area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "    i_area = (intersection[2] - intersection[0]) * (intersection[3] - intersection[1])\n",
    "\n",
    "    # Return the IoMA value\n",
    "    return i_area / min(area1, area2)\n",
    "\n",
    "\n",
    "def add_detections_to_persons(person_boxes, new_boxes, threshold):\n",
    "    \"\"\"Add new detections to existing person boxes based on IoMA.\"\"\"\n",
    "    # Immediately return if there are no new boxes to process\n",
    "    if not new_boxes:\n",
    "        return person_boxes\n",
    "\n",
    "    # Check if there are existing person boxes; if not, add new boxes as new persons directly\n",
    "    if not person_boxes:\n",
    "        return [[n_box] for n_box in new_boxes]\n",
    "\n",
    "    # Create a matrix of IoMA values\n",
    "    ioma_matrix = np.zeros((len(new_boxes), len(person_boxes)))\n",
    "    for i, n_box in enumerate(new_boxes):\n",
    "        for j, person in enumerate(person_boxes):\n",
    "            # Compute the max IoMA of the new box with all boxes of the person\n",
    "            ioma_matrix[i, j] = max(\n",
    "                ioma(n_box[\"bbox\"], p_box[\"bbox\"]) for p_box in person\n",
    "            )\n",
    "\n",
    "    # Continue assigning boxes to persons based on the IoMA matrix\n",
    "    person_assigned = 0\n",
    "    while np.max(ioma_matrix) > threshold:  # and person_assigned < len(person_boxes):\n",
    "        # Find the new box-person pair with the highest IoMA\n",
    "        i, j = np.unravel_index(np.argmax(ioma_matrix, axis=None), ioma_matrix.shape)\n",
    "\n",
    "        # Add the new box to the corresponding person\n",
    "        person_boxes[j].append(new_boxes[i])\n",
    "        person_assigned += 1\n",
    "\n",
    "        # Remove the assigned new box and person from further consideration\n",
    "        ioma_matrix[i, :] = -1  # Invalidate this row (new box)\n",
    "        # ioma_matrix[:, j] = -1  # Invalidate this column (person)\n",
    "\n",
    "    # Add any remaining unassigned new boxes as new persons\n",
    "    for i, row in enumerate(ioma_matrix):\n",
    "        if np.max(row) != -1:  # If this new box hasn't been assigned\n",
    "            person_boxes.append([new_boxes[i]])\n",
    "\n",
    "    return person_boxes\n",
    "\n",
    "\n",
    "def aggregate_person_boxes(res, threshold=0.6):\n",
    "    # Extract and separate person, face, and head bounding boxes from results\n",
    "    persons = [\n",
    "        res.results[i]\n",
    "        for i in range(len(res.results))\n",
    "        if res.results[i][\"label\"] == \"Person\"\n",
    "    ]\n",
    "    heads = [\n",
    "        res.results[i]\n",
    "        for i in range(len(res.results))\n",
    "        if res.results[i][\"label\"] == \"Human head\"\n",
    "    ]\n",
    "    faces = [\n",
    "        res.results[i]\n",
    "        for i in range(len(res.results))\n",
    "        if res.results[i][\"label\"] == \"Human face\"\n",
    "    ]\n",
    "\n",
    "    # Initialize person boxes with each face's box\n",
    "    person_boxes = [[p] for p in faces]\n",
    "\n",
    "    # Add head and person detections to existing person detections\n",
    "    person_boxes = add_detections_to_persons(person_boxes, heads, threshold)\n",
    "    person_boxes = add_detections_to_persons(person_boxes, persons, threshold)\n",
    "    return person_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzer\n",
    "In order to annotate a video, a class known as an Analyzer is required. A child of this class will compute the number of people based on inference results, and annotate each frame with the computed value. This class, called PersonCounter, is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from degirum import _draw_primitives as dp\n",
    "\n",
    "\n",
    "class PersonCounter(dgtools.ResultAnalyzerBase):\n",
    "    \"\"\"Analyzer class to count people using inference results' bounding boxes.\"\"\"\n",
    "\n",
    "    # Compute number of people in a given frame.\n",
    "    def analyze(self, result):\n",
    "        p_box = aggregate_person_boxes(result, threshold=threshold)\n",
    "        result.person_count = len(p_box)\n",
    "\n",
    "    # Annotate a video frame with the number of people in it.\n",
    "    def annotate(self, result, image: ndarray) -> ndarray:\n",
    "\n",
    "        back_color = (\n",
    "            result.overlay_color\n",
    "            if not isinstance(result.overlay_color, list)\n",
    "            else result.overlay_color[0]\n",
    "        )\n",
    "        font_color = dgtools.deduce_text_color(back_color)\n",
    "        label = f\"Number of People: {result.person_count}\"\n",
    "        dgtools.put_text(\n",
    "            image,\n",
    "            label,\n",
    "            (0, image.shape[0]),\n",
    "            corner_position=dgtools.CornerPosition.BOTTOM_LEFT,\n",
    "            bg_color=back_color,\n",
    "            font_color=font_color,\n",
    "        )\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# Instantiate a PersonCounter Analyzer\n",
    "person_counter = PersonCounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate the video source with the PersonCounter Analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgtools.annotate_video(\n",
    "    combined_model, video_source, output_video, analyzers=person_counter\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
