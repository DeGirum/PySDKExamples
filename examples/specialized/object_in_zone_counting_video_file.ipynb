{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## Object detection and object counting in polygon zone: video file annotation\n",
    "\n",
    "This notebook is an example how to use DeGirum PySDK to do object detection and object \n",
    "counting in polygon zone, annotating video file. The annotated video is saved into new file.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option.\n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where you want run inference, video file name, model name, and other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34df11-cbc7-4b00-8994-794a4a6548b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_name: name of the model for running AI inference\n",
    "# video_source: video source for inference\n",
    "#     camera index for local camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "# polygon_zones: zones in which objects need to be counted\n",
    "# class_list: list of classes to be counted\n",
    "# per_class_display: Boolean to specify if per class counts are to be displayed\n",
    "# ann_path: path to save annotated video\n",
    "hw_location = \"@cloud\"\n",
    "model_zoo_url = \"degirum/public\"\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca1_1\"\n",
    "video_source = \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\"\n",
    "polygon_zones = [\n",
    "    [[265, 260], [730, 260], [870, 450], [120, 450]],\n",
    "    [[400, 100], [610, 100], [690, 200], [320, 200]],\n",
    "]\n",
    "class_list = [\"car\", \"motorbike\", \"truck\"]\n",
    "per_class_display = True\n",
    "ann_path = \"temp/object_in_zone_counting_video_file.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1b821-e18e-403b-8147-9f95fc6cfa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "# load model\n",
    "model = dg.load_model(\n",
    "    model_name=model_name, \n",
    "    inference_host_address=hw_location,\n",
    "    zoo_url=model_zoo_url,\n",
    "    token=degirum_tools.get_token(),\n",
    "    overlay_color=[(255,0,0)]\n",
    ")\n",
    "\n",
    "# create zone counter\n",
    "zone_counter = degirum_tools.ZoneCounter(\n",
    "    polygon_zones,\n",
    "    class_list=class_list,\n",
    "    per_class_display=per_class_display,\n",
    "    triggering_position=degirum_tools.AnchorPoint.CENTER,\n",
    ")\n",
    "\n",
    "# attach zone counter to model\n",
    "degirum_tools.attach_analyzers(model, [zone_counter])\n",
    "\n",
    "# annotate video\n",
    "degirum_tools.annotate_video(model, video_source, ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ffa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display result\n",
    "degirum_tools.ipython_display(ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d64fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module degirum.model in degirum:\n",
      "\n",
      "NAME\n",
      "    degirum.model\n",
      "\n",
      "DESCRIPTION\n",
      "    # model.py - DeGirum Python SDK: model implementation\n",
      "    # Copyright DeGirum Corp. 2022\n",
      "    #\n",
      "    # Implements DeGirum model class\n",
      "    #\n",
      "\n",
      "CLASSES\n",
      "    abc.ABC(builtins.object)\n",
      "        Model\n",
      "    \n",
      "    class Model(abc.ABC)\n",
      "     |  Model(model_name: str, model_params: degirum.aiclient.ModelParams, supported_device_types: List[str])\n",
      "     |  \n",
      "     |  Model class. Handles whole inference lifecycle for a single model: input data preprocessing, inference,\n",
      "     |  and postprocessing.\n",
      "     |  \n",
      "     |  !!! note\n",
      "     |  \n",
      "     |      You never construct model objects yourself -- instead you call\n",
      "     |      [degirum.zoo_manager.ZooManager.load_model][] method to create [degirum.model.Model][] instances for you.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Model\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, data)\n",
      "     |      Perform whole inference lifecycle: input data preprocessing, inference and postprocessing.\n",
      "     |      \n",
      "     |      Same as [degirum.model.Model.predict][].\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |      Context manager enter handler.\n",
      "     |  \n",
      "     |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      "     |      Context manager exit handler.\n",
      "     |  \n",
      "     |  __init__(self, model_name: str, model_params: degirum.aiclient.ModelParams, supported_device_types: List[str])\n",
      "     |      Constructor.\n",
      "     |      \n",
      "     |      !!! note\n",
      "     |      \n",
      "     |          You never construct model objects yourself -- instead you call\n",
      "     |          [degirum.zoo_manager.ZooManager.load_model][] method to create [degirum.model.Model][] instances for you.\n",
      "     |  \n",
      "     |  predict(self, data) -> degirum.postprocessor.InferenceResults\n",
      "     |      Perform whole inference lifecycle: input data preprocessing, inference, and postprocessing.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |      \n",
      "     |          data (any): Inference input data. Input data type depends on the model.\n",
      "     |      \n",
      "     |              - If the model expects image data, then the input data is either:\n",
      "     |      \n",
      "     |                  - Input image path string.\n",
      "     |                  - NumPy 3D array of pixels in a form HWC.\n",
      "     |                  where color dimension is native to selected graphical backend (RGB for `'pil'` and BGR for `'opencv'` backend)\n",
      "     |                  - `PIL.Image` object (only for `'pil'` backend).\n",
      "     |      \n",
      "     |              - If the model expects audio data, then the input data is NumPy 1D array with audio data samples.\n",
      "     |      \n",
      "     |              - If the model expects raw tensor data, then the input data is NumPy multidimensional array with shape matching model input.\n",
      "     |      \n",
      "     |              - In case of multi-input model a list of elements of the supported data type is expected.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Inference result object, which allows you to access inference results as a dictionary or as\n",
      "     |              an overlay image if it is supported by the model. For your convenience, all image coordinates in case\n",
      "     |              of detection models are converted from model coordinates to original image coordinates.\n",
      "     |  \n",
      "     |  predict_batch(self, data) -> Iterator[degirum.postprocessor.InferenceResults]\n",
      "     |      Perform whole inference lifecycle for all objects in given iterator object (for example, `list`).\n",
      "     |      \n",
      "     |      Such iterator object should return the same object types which regular [degirum.model.Model.predict][] method accepts.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |      \n",
      "     |          data (iterator): Inference input data iterator object such as list or generator function.\n",
      "     |      \n",
      "     |              Each element returned by this iterator can be one of the following:\n",
      "     |      \n",
      "     |              - A single input data object, in case of single-input model.\n",
      "     |              - A `list` of input data objects, in case of multi-input model.\n",
      "     |              - A `tuple` containing a pair of input data object or a `list` of input data objects as a first element\n",
      "     |              and frame info object as a second element of the `tuple`.\n",
      "     |      \n",
      "     |              The input data object type depends on the model.\n",
      "     |      \n",
      "     |              - If the model expects image data, then the input data object is either:\n",
      "     |                  - Input image path string.\n",
      "     |                  - NumPy 3D array of pixels in a form HWC, where color dimension is native to selected graphical backend\n",
      "     |                  (RGB for `'pil'` and BGR for `'opencv'` backend).\n",
      "     |                  - `PIL.Image` object (only for `'pil'` backend).\n",
      "     |      \n",
      "     |              - If the model expects audio data, then the input data object is NumPy 1D array with audio data samples.\n",
      "     |      \n",
      "     |              - If the model expects raw tensor data, then the input data object is NumPy multidimensional array with shape\n",
      "     |              matching model input.\n",
      "     |      \n",
      "     |              The frame info object is passed to the inference result object unchanged and can be accessed via `info`\n",
      "     |              property of the inference result object.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Generator object which iterates over inference result objects. This allows you directly using the\n",
      "     |              result of [degirum.model.Model.predict_batch][] in `for` loops.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |              for result in model.predict_batch(['image1.jpg','image2.jpg']):\n",
      "     |                  print(result)\n",
      "     |          ```\n",
      "     |  \n",
      "     |  predict_dir(self, path: str, *, recursive: bool = False, extensions=['.jpg', '.jpeg', '.png', '.bmp']) -> Iterator[degirum.postprocessor.InferenceResults]\n",
      "     |      Perform whole inference lifecycle for all files from specified directory matching given file extensions.\n",
      "     |      \n",
      "     |      Supports only single-input models.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          path: Directory name containing files to be processed.\n",
      "     |          recursive: True to recursively walk through all subdirectories in a directory. Default is `False`.\n",
      "     |          extensions (list[str]): Single string or list of strings containing file extension(s) to process.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Generator object to iterate over inference result objects. This allows you directly using the\n",
      "     |              result of [degirum.model.Model.predict_dir][] in `for` loops.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      \n",
      "     |          ```python\n",
      "     |              for result in model.predict_dir('./some_path'):\n",
      "     |                  print(result)\n",
      "     |          ```\n",
      "     |  \n",
      "     |  reset_time_stats(self)\n",
      "     |      Reset inference time statistics.\n",
      "     |      \n",
      "     |      [degirum.model.Model.time_stats][] method will return empty dictionary after this call.\n",
      "     |  \n",
      "     |  time_stats(self) -> dict\n",
      "     |      Query inference time statistics.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Dictionary containing time statistic objects.\n",
      "     |      \n",
      "     |              - A key in that dictionary is a string description of a particular inference step.\n",
      "     |              - Each statistic object keeps min, max, and average values in milliseconds, accumulated over all\n",
      "     |              inferences performed on this model since the model creation of last call of statistic reset\n",
      "     |              method [degirum.model.Model.reset_time_stats][].\n",
      "     |              - Time statistics are accumulated only when [degirum.model.Model.measure_time][] property is set to `True`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  devices_available\n",
      "     |      The list of inference device indices which can be used for model inference.\n",
      "     |  \n",
      "     |  label_dictionary\n",
      "     |      Get model class label dictionary.\n",
      "     |      \n",
      "     |      Each dictionary element is key-value pair, where the key is the class ID\n",
      "     |      and the value is the class label string.\n",
      "     |  \n",
      "     |  model_info\n",
      "     |      Return model information object to provide read-only access to model parameters.\n",
      "     |      \n",
      "     |      New deep copy is created each time.\n",
      "     |  \n",
      "     |  supported_device_types\n",
      "     |      The list of supported device types in format `<runtime>/<device>` for this model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  custom_postprocessor\n",
      "     |      Custom postprocessor class. When not None, the object of this class is returned as inference result.\n",
      "     |      Such custom postprocessor classes must be inherited from [degirum.postprocessor.InferenceResults][] class.\n",
      "     |  \n",
      "     |  device_type\n",
      "     |      The type of the device to be used for model inference in a format `<runtime>/<device>`\n",
      "     |      \n",
      "     |      Setter accepts either a string which specifies single device in a format `<runtime>/<device>`\n",
      "     |      or it can be a list of such strings. In this case the first supported device type\n",
      "     |      from the list will be selected.\n",
      "     |      \n",
      "     |      Supported device types can be obtained by [degirum.model.Model.supported_device_types][] property.\n",
      "     |      \n",
      "     |      Getter returns currently selected device type.\n",
      "     |  \n",
      "     |  devices_selected\n",
      "     |      The list of inference device indices selected for model inference.\n",
      "     |  \n",
      "     |  eager_batch_size\n",
      "     |      The size of the batch (number of consecutive frames before this model is switched\n",
      "     |      to another model during batch predict) to be used by device scheduler when inferencing this model.\n",
      "     |  \n",
      "     |  frame_queue_depth\n",
      "     |      The depth of the model prediction queue. When the queue size reaches this value,\n",
      "     |      the next prediction call will block until there will be space in the queue.\n",
      "     |  \n",
      "     |  image_backend\n",
      "     |      Graphical library (*backend*) to use for graphical tasks -- one of `'pil'`, `'opencv'`, `'auto'`\n",
      "     |      \n",
      "     |      `'auto'` means try OpenCV first, if not installed, try PIL.\n",
      "     |  \n",
      "     |  inference_timeout_s\n",
      "     |      The maximum time in seconds to wait for inference result from the model.\n",
      "     |  \n",
      "     |  input_crop_percentage\n",
      "     |      Percentage of image to crop around. Valid range: `[0..1]`.\n",
      "     |  \n",
      "     |  input_image_format\n",
      "     |      Defines the image format for model inputs of image type -- one of `'JPEG'` or `'RAW'`.\n",
      "     |  \n",
      "     |  input_letterbox_fill_color\n",
      "     |      Image fill color in case of `'letterbox'` padding (see [degirum.model.Model.input_pad_method][]\n",
      "     |      property for details).\n",
      "     |      \n",
      "     |      3-element RGB tuple.\n",
      "     |  \n",
      "     |  input_numpy_colorspace\n",
      "     |      Input image colorspace -- one of `'RGB'`, `'BGR'`, or `'auto'`.\n",
      "     |      \n",
      "     |      This parameter is used **only** to identify colorspace for NumPy arrays.\n",
      "     |      \n",
      "     |      `'auto'` translates to `'BGR'` for `opencv` backend, and to `'RGB'` for `pil` backend.\n",
      "     |  \n",
      "     |  input_pad_method\n",
      "     |      Input image pad method -- one of `'stretch'`,  `'letterbox'`, `'crop-first'`, or `'crop-last'`.\n",
      "     |      \n",
      "     |      - In case of `'stretch'`, the input image will be resized to the model input size **without** preserving aspect ratio.\n",
      "     |      - In case of `'letterbox'`, the input image will be resized to the model input size preserving aspect ratio.\n",
      "     |      - In case of `'crop-first'`, the input image will be cropped to input_crop_percentage around the center and then resized.\n",
      "     |      - In the case of 'crop-last', if the model's input dimensions are square, the image is resized with its smaller side matching the model dimension, preserving the aspect ratio. If the dimensions are rectangular, the image is resized and stretched to fit the model's input dimensions. After resizing, the image is cropped to the model's input dimensions and aspect ratio based on the 'input_crop_percentage' property.\n",
      "     |      \n",
      "     |      The voids will be filled with solid color specified by `input_letterbox_fill_color` property.\n",
      "     |      In all cases [degirum.model.Model.input_resize_method][] property specifies the algorithm for resizing.\n",
      "     |  \n",
      "     |  input_resize_method\n",
      "     |      Input image resize method -- one of `'nearest'`, `'bilinear'`, `'area'`, `'bicubic'`, or `'lanczos'`.\n",
      "     |  \n",
      "     |  input_shape\n",
      "     |      Input tensor shapes. List of tensor shapes per input.\n",
      "     |      \n",
      "     |      Each element of that list is another list containing tensor dimensions, slowest dimension first:\n",
      "     |      \n",
      "     |          - if InputShape model parameter is specified, its value is used.\n",
      "     |          - otherwise, InputN/H/W/C model parameters are used as [InputN, InputH, InputW, InputC] list.\n",
      "     |  \n",
      "     |  measure_time\n",
      "     |      Flag to enable measuring and collecting inference time statistics.\n",
      "     |      \n",
      "     |      Call [degirum.model.Model.time_stats][] to query accumulated inference time statistics.\n",
      "     |  \n",
      "     |  non_blocking_batch_predict\n",
      "     |      Flag to control the behavior of the generator object returned by `predict_batch()` method.\n",
      "     |      \n",
      "     |      - When the flag is set to `True`, the generator accepts `None` from the inference input data\n",
      "     |      iterator object (passed as `data` parameter): If `None` is returned, the model predict step is skipped for\n",
      "     |      this iteration. Also, when no inference results are available in the result queue at this iteration,\n",
      "     |      the generator yields `None` result.\n",
      "     |      \n",
      "     |      - When the flag is set to `False` (default value), the generator does not allow `None` to be returned from\n",
      "     |      the inference input data iterator object: If `None` is returned, an exception is raised.\n",
      "     |      Also, when no inference results are available in the result queue at this iteration,\n",
      "     |      the generator continues to the next iteration of the input data iterator.\n",
      "     |      \n",
      "     |      - Setting this flag to `True` allows using `predict_batch()` generator in a non-blocking manner, assuming\n",
      "     |      the design of input data iterator object is also non-blocking, i.e., returning `None` when no data is\n",
      "     |      available instead of waiting for the data. Every next element request from the generator will not block\n",
      "     |      the execution waiting for either input data or inference results, returning `None` when no results are\n",
      "     |      available.\n",
      "     |  \n",
      "     |  output_class_set\n",
      "     |      Labels filter: list of class labels/category IDs to be included in inference results.\n",
      "     |      \n",
      "     |      !!! note\n",
      "     |      \n",
      "     |          You can use [degirum.model.Model.label_dictionary][] property to obtain a list of model classes.\n",
      "     |  \n",
      "     |  output_confidence_threshold\n",
      "     |      Confidence threshold used in inference result post-processing.\n",
      "     |      \n",
      "     |      Valid range: `[0..1]`.\n",
      "     |      \n",
      "     |      Only objects with scores higher than this threshold are reported.\n",
      "     |      \n",
      "     |      !!! note\n",
      "     |      \n",
      "     |          For classification models if [degirum.model.Model.output_top_k][] parameter is set to non-zero value,\n",
      "     |          then it supersedes this threshold -- [degirum.model.Model.output_top_k][] highest score classes are always reported.\n",
      "     |  \n",
      "     |  output_max_classes_per_detection\n",
      "     |      Max Detections Per Class number used in inference result post-processing, and specifies the maximum number\n",
      "     |      of highest probability classes per anchor to be processed during the non-max suppression process for fast algorithm.\n",
      "     |      \n",
      "     |      Applicable only for detection models.\n",
      "     |  \n",
      "     |  output_max_detections\n",
      "     |      Max Detection number used in inference result post-processing, and specifies the total maximum objects\n",
      "     |      of number to be detected.\n",
      "     |      \n",
      "     |      Applicable only for detection models.\n",
      "     |  \n",
      "     |  output_max_detections_per_class\n",
      "     |      Max Detections Per Class number used in inference result post-processing, and specifies the maximum number\n",
      "     |      of objects to keep during per class non-max suppression process for regular algorithm.\n",
      "     |      \n",
      "     |      Applicable only for detection models.\n",
      "     |  \n",
      "     |  output_nms_threshold\n",
      "     |      Non-Max Suppression (NMS) threshold used in inference result post-processing.\n",
      "     |      \n",
      "     |      Valid range: `[0..1]`.\n",
      "     |      \n",
      "     |      Applicable only for models which utilize NMS algorithm.\n",
      "     |  \n",
      "     |  output_pose_threshold\n",
      "     |      Pose detection threshold used in inference result post-processing.\n",
      "     |      \n",
      "     |      Valid range: `[0..1]`.\n",
      "     |      \n",
      "     |      Applicable only for pose detection models.\n",
      "     |  \n",
      "     |  output_postprocess_type\n",
      "     |      Inference result post-processing type.\n",
      "     |      \n",
      "     |      You may set it to `'None'` to bypass post-processing.\n",
      "     |  \n",
      "     |  output_top_k\n",
      "     |      The number of classes with highest scores to report for classification models.\n",
      "     |      \n",
      "     |      When set to `0`, then report all classes with scores greater than [degirum.model.Model.output_confidence_threshold][].\n",
      "     |  \n",
      "     |  output_use_regular_nms\n",
      "     |      Use Regular NMS value used in inference result post-processing and specifies the algorithm to use for\n",
      "     |      detection postprocessing.\n",
      "     |      \n",
      "     |      If value is `True`, regular Non-Max suppression algorithm is used -- NMS is calculated for each class\n",
      "     |      separately and after that all results are merged.\n",
      "     |      \n",
      "     |      If value is `False`, fast Non-Max suppression algorithm is used -- NMS is calculated for all classes\n",
      "     |      simultaneously.\n",
      "     |  \n",
      "     |  overlay_alpha\n",
      "     |      Alpha-blend weight for inference results drawing on overlay image.\n",
      "     |      \n",
      "     |      `float` number in range `[0..1]`.\n",
      "     |      \n",
      "     |      See [degirum.postprocessor.InferenceResults.image_overlay][] for more details.\n",
      "     |  \n",
      "     |  overlay_color\n",
      "     |      Color for inference results drawing on overlay image.\n",
      "     |      \n",
      "     |      3-element RGB tuple or list of 3-element RGB tuples.\n",
      "     |      \n",
      "     |      The `overlay_color` property is used to define the color to draw overlay details. In the case of a single RGB tuple,\n",
      "     |      the corresponding color is used to draw all the overlay data: points, boxes, labels, segments, etc.\n",
      "     |      In the case of a list of RGB tuples the behavior depends on the model type:\n",
      "     |      \n",
      "     |      - For classification models different colors from the list are used to draw labels of different classes.\n",
      "     |      - For detection models different colors are used to draw labels *and boxes* of different classes.\n",
      "     |      - For pose detection models different colors are used to draw keypoints of different persons.\n",
      "     |      - For segmentation models different colors are used to highlight segments of different classes.\n",
      "     |      \n",
      "     |      If the list size is less than the number of classes of the model, then `overlay_color` values are used cyclically,\n",
      "     |      for example, for three-element list it will be `overlay_color[0]`, then `overlay_color[1]`, `overlay_color[2]`,\n",
      "     |      and again `overlay_color[0]`.\n",
      "     |      \n",
      "     |      The default value of `overlay_color` is a single RBG tuple of yellow color for all model types except segmentation models.\n",
      "     |      For segmentation models it is the list of RGB tuples with the list size equal to the number of model classes.\n",
      "     |      You can use [degirum.model.Model.label_dictionary][] property to obtain a list of model classes.\n",
      "     |      Each color is automatically assigned to look pretty and different from other colors in the list.\n",
      "     |  \n",
      "     |  overlay_font_scale\n",
      "     |      Font scale for inference results drawing on overlay image.\n",
      "     |      \n",
      "     |      `float` positive number.\n",
      "     |      \n",
      "     |      See [degirum.postprocessor.InferenceResults.image_overlay][] for more details.\n",
      "     |  \n",
      "     |  overlay_line_width\n",
      "     |      Line width for inference results drawing on overlay image.\n",
      "     |      \n",
      "     |      See [degirum.postprocessor.InferenceResults.image_overlay][] for more details.\n",
      "     |  \n",
      "     |  overlay_show_labels\n",
      "     |      Flag to enable/disable drawing class labels on overlay image.\n",
      "     |      \n",
      "     |      See [degirum.postprocessor.InferenceResults.image_overlay][] for more details.\n",
      "     |  \n",
      "     |  overlay_show_probabilities\n",
      "     |      Flag to enable/disable drawing class probabilities on overlay image.\n",
      "     |      \n",
      "     |      See [degirum.postprocessor.InferenceResults.image_overlay][] for more details.\n",
      "     |  \n",
      "     |  save_model_image\n",
      "     |      Flag to enable/disable saving of model input image in inference results.\n",
      "     |      \n",
      "     |      Model input image is the image converted to AI model input specifications as raw binary array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'_predict_handler', 'devices_availabl...\n",
      "\n",
      "FUNCTIONS\n",
      "    dg_check_error = check_error(...) method of builtins.PyCapsule instance\n",
      "        check_error(arg0: object) -> str\n",
      "        \n",
      "        Check given AI server Json response for run-time errors\n",
      "    \n",
      "    server_system_info = system_info(...) method of builtins.PyCapsule instance\n",
      "        system_info(arg0: str) -> json\n",
      "        \n",
      "        Return host system information dictionary\n",
      "\n",
      "DATA\n",
      "    Callable = typing.Callable\n",
      "        Callable type; Callable[[int], str] is a function of (int) -> str.\n",
      "        \n",
      "        The subscription syntax must always be used with exactly two\n",
      "        values: the argument list and the return type.  The argument list\n",
      "        must be a list of types or ellipsis; the return type must be a single type.\n",
      "        \n",
      "        There is no syntax to indicate optional or keyword arguments,\n",
      "        such function types are rarely used as callback types.\n",
      "    \n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    Iterator = typing.Iterator\n",
      "        A generic version of collections.abc.Iterator.\n",
      "    \n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Optional = typing.Optional\n",
      "        Optional type.\n",
      "        \n",
      "        Optional[X] is equivalent to Union[X, None].\n",
      "    \n",
      "    Union = typing.Union\n",
      "        Union type; Union[X, Y] means either X or Y.\n",
      "        \n",
      "        To define a union, use e.g. Union[int, str].  Details:\n",
      "        - The arguments must be types and there must be at least one.\n",
      "        - None as an argument is a special case and is replaced by\n",
      "          type(None).\n",
      "        - Unions of unions are flattened, e.g.::\n",
      "        \n",
      "            Union[Union[int, str], float] == Union[int, str, float]\n",
      "        \n",
      "        - Unions of a single argument vanish, e.g.::\n",
      "        \n",
      "            Union[int] == int  # The constructor actually returns int\n",
      "        \n",
      "        - Redundant arguments are skipped, e.g.::\n",
      "        \n",
      "            Union[int, str, int] == Union[int, str]\n",
      "        \n",
      "        - When comparing unions, the argument order is ignored, e.g.::\n",
      "        \n",
      "            Union[int, str] == Union[str, int]\n",
      "        \n",
      "        - You cannot subclass or instantiate a union.\n",
      "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
      "    \n",
      "    logger = <Logger degirum.model (WARNING)>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\shashichilappagari\\anaconda3\\envs\\supervision\\lib\\site-packages\\degirum\\model.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a17cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (supervision)",
   "language": "python",
   "name": "supervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
