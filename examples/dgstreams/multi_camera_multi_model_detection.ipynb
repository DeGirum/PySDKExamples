{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Source and Multi-Model AI Inference\n",
    "This notebook is an example how to perform AI inferences of multiple models processing multiple video streams.\n",
    "Each video stream is fed to every model. Each model processes frames from every video stream in multiplexing manner.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option. \n",
    "\n",
    "You also need to specify your cloud API access token in [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "The script can use a web camera(s) or local camera(s) connected to the machine running this code  or it can use video file(s).\n",
    "The camera index or URL or video file path should be specified in the code below by assigning `video_sources` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify video sources and AI model names here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     "@cloud" to use DeGirum cloud\n",
    "#     "@local: to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# video_sources: list of video sources\n",
    "#     camera index for local web camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_names: list of AI models to use for inferences (NOTE: they should have the same input size)\n",
    "# allow_frame_drop:\n",
    "#     when True, we drop video frames in case when AI performance is not enough to work in real time\n",
    "#     when False, we buffer video frames to keep up with AI performance\n",
    "hw_location = \"@cloud\"\n",
    "video_sources = [\n",
    "    \"../../images/Traffic.mp4\",\n",
    "    \"../../images/TrafficHD.mp4\",\n",
    "]\n",
    "model_zoo_url = \"https://cs.degirum.com/degirum/public\"\n",
    "model_names = [\n",
    "    \"yolo_v5s_hand_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5s_face_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5n_car_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5s_person_det--512x512_quant_n2x_orca1_1\",\n",
    "]\n",
    "allow_frame_drop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "# configure for Google Colab\n",
    "degirum_tools.configure_colab()\n",
    "# connect to AI inference engine getting the token from env.ini file\n",
    "zoo = dg.connect(hw_location, model_zoo_url, degirum_tools.get_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from degirum_tools import streams as dgstreams\n",
    "\n",
    "c = dgstreams.Composition()\n",
    "\n",
    "batch_size = len(\n",
    "    video_sources\n",
    ")  # set AI server batch size equal to the # of video sources for lowest latency\n",
    "\n",
    "# create PySDK AI model objects\n",
    "models = []\n",
    "for mi, model_name in enumerate(model_names):\n",
    "    model = zoo.load_model(model_name)\n",
    "    model.measure_time = True\n",
    "    model.eager_batch_size = batch_size\n",
    "    model.frame_queue_depth = batch_size\n",
    "    models.append(model)\n",
    "\n",
    "# check that all models have the same input configuration\n",
    "models_have_same_input = True\n",
    "for model in models[1:]:\n",
    "    if (\n",
    "        type(model._preprocessor) != type(models[0]._preprocessor)\n",
    "        or model.model_info.InputH != models[0].model_info.InputH\n",
    "        or model.model_info.InputW != models[0].model_info.InputW\n",
    "    ):\n",
    "        models_have_same_input = False\n",
    "\n",
    "resizers = []\n",
    "\n",
    "# create video sources and image resizers\n",
    "# (we use separate resizers to do resize only once per source when possible, to improve performance),\n",
    "# connect each resizer to corresponding video source\n",
    "for src in video_sources:\n",
    "    source = c.add(dgstreams.VideoSourceGizmo(src))\n",
    "    if models_have_same_input:\n",
    "        resizer = c.add(\n",
    "            dgstreams.AiPreprocessGizmo(\n",
    "                models[0], stream_depth=2, allow_drop=allow_frame_drop\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        resizer = c.add(dgstreams.FanoutGizmo(allow_drop=allow_frame_drop))\n",
    "\n",
    "    resizer.connect_to(source)  # connect resizer to video source\n",
    "    resizers.append(resizer)\n",
    "\n",
    "# create result combiner\n",
    "combiner = c.add(dgstreams.AiResultCombiningGizmo(len(models)))\n",
    "\n",
    "# create multi-input detector gizmos,\n",
    "# connect each detector gizmo to every resizer gizmo,\n",
    "# connect result combiner gizmo to each detector gizmo\n",
    "for mi, model in enumerate(models):\n",
    "    # create AI gizmo (aka detector) from the model\n",
    "    detector = c.add(\n",
    "        dgstreams.AiSimpleGizmo(model, stream_depth=2, inp_cnt=len(video_sources))\n",
    "    )\n",
    "\n",
    "    # connect detector gizmo to each resizer gizmo\n",
    "    for fi, resizer in enumerate(resizers):\n",
    "        detector.connect_to(resizer, fi)\n",
    "\n",
    "    # connect result combiner gizmo to detector gizmo\n",
    "    combiner.connect_to(detector, mi)\n",
    "\n",
    "# create multi-window video multiplexing display gizmo\n",
    "# and connect it to combiner gizmo\n",
    "win_captions = [f\"Camera #{i}: {str(src)}\" for i, src in enumerate(video_sources)]\n",
    "display = c.add(\n",
    "    dgstreams.VideoDisplayGizmo(\n",
    "        win_captions, show_ai_overlay=True, show_fps=True, multiplex=True\n",
    "    )\n",
    ")\n",
    "display.connect_to(combiner)\n",
    "\n",
    "# start composition\n",
    "c.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
