{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## AI Inference on a video stream\n",
    "This notebook is a simple example on how to use DeGirum PySDK to do AI inference on a video stream.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to specify the appropriate target option. \n",
    "\n",
    "You also need to specify your cloud API access token in [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "You can change camera_id to index of a local webcamera, or URL of an RTSP stream, or URL of a YouTube video, or path to another video file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76681f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965103da-b8bb-4a02-af4f-6b8a97c58e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where you want to run your inferences, video source, model zoo url, and model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11422340",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    target: where you want to run inference\n",
    "        @cloud to use DeGirum cloud\n",
    "        @local to run on local machine\n",
    "        IP address for AI server inference\n",
    "    model_zoo_url: url/path for model zoo\n",
    "        cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "        '': ai server serving models from local folder\n",
    "        path to json file: single model zoo in case of @local inference\n",
    "    model_name: name of the model for running AI inference\n",
    "    camera_id: video source for inference\n",
    "        camera index for local web camera\n",
    "        URL of RTSP stream\n",
    "        URL of YouTube Video\n",
    "        path to video file (mp4 etc)\n",
    "'''\n",
    "target=\"@cloud\"\n",
    "model_zoo_url = 'https://cs.degirum.com/degirum/public'\n",
    "model_name= 'mobilenet_v2_ssd_coco--300x300_quant_n2x_orca1_1'\n",
    "camera_id = '../../Images/Traffic.mp4'        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c31690",
   "metadata": {},
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "# configure for Google Colab\n",
    "degirum_tools.configure_colab() \n",
    "# connect to AI inference engine getting token from env.ini file\n",
    "zoo = dg.connect(target, model_zoo_url, degirum_tools.get_token())\n",
    "# load object detection AI model for DeGirum Orca AI accelerator\n",
    "model = zoo.load_model(model_name,\n",
    "                       overlay_show_probabilities=True\n",
    "                       )\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "with degirum_tools.Display(\"AI Camera\") as display:    \n",
    "    for res in degirum_tools.predict_stream(model, camera_id):\n",
    "        display.show(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
