{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f257328f",
   "metadata": {},
   "source": [
    "## Example script illustrating sound classification on audio stream\n",
    "This notebook is an example how to use DeGirum PySDK to do sound classification AI inference of an audio stream from local microphone.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "**pyaudio package with portaudio is required to run this sample.**\n",
    "\n",
    "**Access to microphone is required to run this sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227c649-6c23-41d1-a6df-4247f4a6a480",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef133f4-8197-4de5-a44e-c76dbbd39a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "degirum_tools.configure_colab() # configure for Google Colab\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "target = dg.CLOUD # <-- on the Cloud Platform\n",
    "# target = degirum_tools.get_ai_server_hostname() # <-- on AI Server deployed in your LAN\n",
    "# target = dg.LOCAL # <-- on ORCA accelerator installed on this computer\n",
    "\n",
    "# connect to AI inference engine getting zoo URL and token from env.ini file\n",
    "zoo = dg.connect(target, degirum_tools.get_cloud_zoo_url(), degirum_tools.get_token())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86162da-d4bc-42d6-b839-b10025306796",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a1753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd775c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load YAMNET sound classification model for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "model = zoo.load_model(\"mobilenet_v1_yamnet_sound_cls--96x64_quant_n2x_orca1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db989e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abort = False  # stream abort flag\n",
    "N = 5  # inference results history depth\n",
    "history = []  # list of N consecutive inference results\n",
    "\n",
    "sampling_rate_hz = model.model_info.InputSamplingRate[0]\n",
    "read_buf_size = model.model_info.InputWaveformSize[0] // 2  # to have 50% overlap\n",
    "\n",
    "# Acquire model input stream object\n",
    "with degirum_tools.open_audio_stream(sampling_rate_hz, read_buf_size) as stream:\n",
    "    #\n",
    "    # AI prediction loop.\n",
    "    # emit keyboard typing sound to stop\n",
    "    #\n",
    "    for res in model.predict_batch(\n",
    "        degirum_tools.audio_overlapped_source(stream, lambda: abort)\n",
    "    ):\n",
    "        # add top inference result to history\n",
    "        history.insert(0, f\"{res.results[0]['label']}: {res.results[0]['score']}\")\n",
    "        if len(history) > N:  # keep only N last elements in history\n",
    "            history.pop()\n",
    "\n",
    "        clear_output(wait=True)  # clear Jupyter output cell\n",
    "        for m in history:  # print history\n",
    "            print(m)\n",
    "\n",
    "        if res.results[0][\"label\"] == \"Typing\":  # check for stop condition\n",
    "            abort = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63eee6-e66d-4a23-bce6-1d4bcf41cce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
