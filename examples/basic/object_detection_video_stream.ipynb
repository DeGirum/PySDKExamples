{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "## AI Inference on a video stream\n",
    "This notebook is a simple example of how to use DeGirum PySDK to do AI inference on a video stream.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `target` option. \n",
    "\n",
    "You also need to specify your cloud API access token in [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "You can change `video_source` to index of a local webcamera, or URL of an RTSP stream, or URL of a YouTube video, or path to another video file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76681f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965103da-b8bb-4a02-af4f-6b8a97c58e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where you want to run your inferences, model zoo url, model name and video source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11422340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     @cloud to use DeGirum cloud\n",
    "#     @local to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_name: name of the model for running AI inference\n",
    "# video_source: video source for inference\n",
    "#     camera index for local web camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "hw_location='@cloud'\n",
    "model_zoo_url = 'https://cs.degirum.com/degirum/ultralytics_v6'\n",
    "model_name= 'yolov8n_silu_coco--640x640_float_openvino_cpu_1'\n",
    "video_source = '../../images/example_video.mp4'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "# configure for Google Colab\n",
    "degirum_tools.configure_colab() \n",
    "# connect to AI inference engine getting token from env.ini file\n",
    "zoo = dg.connect(hw_location, model_zoo_url, degirum_tools.get_token())\n",
    "# load object detection AI model for DeGirum Orca AI accelerator\n",
    "model = zoo.load_model(model_name,\n",
    "                       overlay_show_probabilities=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c31690",
   "metadata": {},
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea1e8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened video stream '../../images/example_video.mp4'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "with degirum_tools.Display(\"AI Camera\") as display:    \n",
    "    for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "        display.show(inference_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.overlay_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened video stream '../../images/example_video.mp4'\n"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "import numpy as np\n",
    "box_annotator = sv.BoundingBoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "blur_annotator = sv.BlurAnnotator()\n",
    "with degirum_tools.Display(\"AI Camera\") as display:    \n",
    "    for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "        detections = sv.Detections(np.array([obj[\"bbox\"] for obj in res.results]), \n",
    "                                   confidence=np.array([obj[\"score\"] for obj in res.results]),\n",
    "                                   class_id=np.array([obj[\"category_id\"] for obj in res.results]))\n",
    "        labels = [ f\"{model.label_dictionary[class_id]} {confidence:0.2f}\"\n",
    "          for _, _, confidence, class_id, track_id in detections\n",
    "         ]\n",
    "        annotated_frame = box_annotator.annotate(scene=res.image.copy(),\n",
    "                                                 detections=detections\n",
    "                                                 )\n",
    "        annotated_frame = blur_annotator.annotate(scene=annotated_frame,\n",
    "                                                 detections=detections\n",
    "                                                 )\n",
    "        annotated_labeled_frame = label_annotator.annotate(scene=annotated_frame,\n",
    "                                                           detections=detections,\n",
    "                                                           labels=labels\n",
    "                                                           )\n",
    "        display.show(annotated_labeled_frame)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "class ZoneCounter:\n",
    "    \"\"\"\n",
    "    Class to count detected object bounding boxes in polygon zones\n",
    "    \"\"\"\n",
    "\n",
    "    # Triggering position within the bounding box\n",
    "    CENTER = sv.Position.CENTER\n",
    "    CENTER_LEFT = sv.Position.CENTER_LEFT\n",
    "    CENTER_RIGHT = sv.Position.CENTER_RIGHT\n",
    "    TOP_CENTER = sv.Position.TOP_CENTER\n",
    "    TOP_LEFT = sv.Position.TOP_LEFT\n",
    "    TOP_RIGHT = sv.Position.TOP_RIGHT\n",
    "    BOTTOM_LEFT = sv.Position.BOTTOM_LEFT\n",
    "    BOTTOM_CENTER = sv.Position.BOTTOM_CENTER\n",
    "    BOTTOM_RIGHT = sv.Position.BOTTOM_RIGHT\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        zone_polygons,        \n",
    "        zone_colors,\n",
    "        *,        \n",
    "        triggering_position=BOTTOM_CENTER,\n",
    "        window_name=None,\n",
    "    ):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            zone_polygons - list of polygons to count objects in; each polygon is a list of points (x,y)\n",
    "            triggering_position: the position within the bounding box that triggers the zone\n",
    "            window_name - optional OpenCV window name to configure for interactive zone adjustment\n",
    "        \"\"\"\n",
    "\n",
    "        self._wh = None\n",
    "        self._zones = None\n",
    "        self._win_name = window_name\n",
    "        self._mouse_callback_installed = False\n",
    "        self._triggering_position = triggering_position\n",
    "        self._polygons = [\n",
    "            np.array(polygon, dtype=np.int32) for polygon in zone_polygons\n",
    "        ]\n",
    "        self._zone_annotators=None\n",
    "        self._zone_colors=zone_colors\n",
    "\n",
    "    def _lazy_init(self, result):\n",
    "        \"\"\"\n",
    "        Complete deferred initialization steps\n",
    "            - initialize polygon zones from model result object\n",
    "            - install mouse callback\n",
    "        \"\"\"\n",
    "        if self._zones is None:\n",
    "            self._wh = (result.image.shape[1], result.image.shape[0])\n",
    "            print(self._wh)\n",
    "            self._zones = [\n",
    "                sv.PolygonZone(polygon, self._wh, self._triggering_position)\n",
    "                for polygon in self._polygons\n",
    "            ]\n",
    "        if self._zone_annotators is None:\n",
    "            self._zone_annotators = [\n",
    "                sv.PolygonZoneAnnotator(zone, color)\n",
    "                for zone, color in zip(self._zones,self._zone_colors)\n",
    "            ]\n",
    "        if not self._mouse_callback_installed and self._win_name is not None:\n",
    "            self._install_mouse_callback()\n",
    "\n",
    "    def window_attach(self, win_name):\n",
    "        \"\"\"Attach OpenCV window for interactive zone adjustment by installing mouse callback\n",
    "        Args:\n",
    "            win_name - OpenCV window name to attach to\n",
    "        \"\"\"\n",
    "\n",
    "        self._win_name = win_name\n",
    "        self._mouse_callback_installed = False\n",
    "\n",
    "    def count(self, res, detections):\n",
    "        \"\"\"\n",
    "        Count detected object bounding boxes in polygon zones\n",
    "\n",
    "        Args:\n",
    "            result - model result object\n",
    "        Returns:\n",
    "            list of object counts found in each polygon zone\n",
    "        \"\"\"\n",
    "        self._lazy_init(res)\n",
    "        if self._zones is not None:\n",
    "            return [\n",
    "                (zone.trigger(detections).sum() if len(detections) > 0 else 0)\n",
    "                for zone in self._zones\n",
    "            ]\n",
    "        return None\n",
    "    def annotate(self, image, zone_counts):\n",
    "        \"\"\"\n",
    "        Display polygon zones and counts on given image\n",
    "\n",
    "        Args:\n",
    "            result - result object to take display settings from\n",
    "            image - image to display on\n",
    "            zone_counts - list of object counts found in each polygon zone\n",
    "        Returns:\n",
    "            annotated image\n",
    "        \"\"\"\n",
    "        if self._zone_annotators is not None:\n",
    "            print(image.shape)\n",
    "            image= [\n",
    "                (zone_annotator.annotate(image, zone_count))\n",
    "                for zone_annotator, zone_count in zip(self._zone_annotators, zone_counts)\n",
    "            ]\n",
    "            print(image)\n",
    "        return image   \n",
    "    def display(self, result, image, zone_counts):\n",
    "        \"\"\"\n",
    "        Display polygon zones and counts on given image\n",
    "\n",
    "        Args:\n",
    "            result - result object to take display settings from\n",
    "            image - image to display on\n",
    "            zone_counts - list of object counts found in each polygon zone\n",
    "        Returns:\n",
    "            annotated image\n",
    "        \"\"\"\n",
    "\n",
    "        def color_complement(color):\n",
    "            adj_color = (color[0] if isinstance(color, list) else color)[::-1]\n",
    "            return tuple([255 - c for c in adj_color])\n",
    "\n",
    "        zone_color = color_complement(result.overlay_color)\n",
    "        background_color = color_complement(result.overlay_fill_color)\n",
    "\n",
    "        for zi in range(len(self._polygons)):\n",
    "            cv2.polylines(\n",
    "                image, [self._polygons[zi]], True, zone_color, result.overlay_line_width\n",
    "            )\n",
    "            degirum_tools.Display.put_text(\n",
    "                image,\n",
    "                f\"Zone {zi}: {zone_counts[zi]}\",\n",
    "                self._polygons[zi][0],\n",
    "                zone_color,\n",
    "                background_color,\n",
    "                cv2.FONT_HERSHEY_PLAIN,\n",
    "                result.overlay_font_scale,\n",
    "            )\n",
    "        return image\n",
    "    def _mouse_callback(event, x, y, flags, self):\n",
    "        \"\"\"Mouse callback for OpenCV window for interactive zone operations\"\"\"\n",
    "\n",
    "        click_point = np.array((x, y))\n",
    "\n",
    "        def zone_update():\n",
    "            idx = self._gui_state[\"update\"]\n",
    "            if idx >= 0 and self._wh is not None:\n",
    "                self._zones[idx] = sv.PolygonZone(\n",
    "                    self._polygons[idx], self._wh, self._triggering_position\n",
    "                )\n",
    "\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            for idx, polygon in enumerate(self._polygons):\n",
    "                if cv2.pointPolygonTest(polygon, (x, y), False) > 0:\n",
    "                    zone_update()\n",
    "                    self._gui_state[\"dragging\"] = polygon\n",
    "                    self._gui_state[\"offset\"] = click_point\n",
    "                    self._gui_state[\"update\"] = idx\n",
    "                    break\n",
    "\n",
    "        if event == cv2.EVENT_RBUTTONDOWN:\n",
    "            for idx, polygon in enumerate(self._polygons):\n",
    "                for pt in polygon:\n",
    "                    if np.linalg.norm(pt - click_point) < 10:\n",
    "                        zone_update()\n",
    "                        self._gui_state[\"dragging\"] = pt\n",
    "                        self._gui_state[\"offset\"] = click_point\n",
    "                        self._gui_state[\"update\"] = idx\n",
    "                        break\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if self._gui_state[\"dragging\"] is not None:\n",
    "                delta = click_point - self._gui_state[\"offset\"]\n",
    "                self._gui_state[\"dragging\"] += delta\n",
    "                self._gui_state[\"offset\"] = click_point\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP or event == cv2.EVENT_RBUTTONUP:\n",
    "            self._gui_state[\"dragging\"] = None\n",
    "            zone_update()\n",
    "            self._gui_state[\"update\"] = -1\n",
    "\n",
    "    def _install_mouse_callback(self):\n",
    "        try:\n",
    "            cv2.setMouseCallback(self._win_name, ZoneCounter._mouse_callback, self)  # type: ignore[attr-defined]\n",
    "            self._gui_state = {\"dragging\": None, \"update\": -1}\n",
    "            self._mouse_callback_installed = True\n",
    "        except Exception:\n",
    "            pass  # ignore errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictStreamResult:\n",
    "    def __init__(self, res, detections, labels, zone_counter=None, zone_counts=None):\n",
    "        self._result = res\n",
    "        self._detections=detections\n",
    "        self._labels=labels\n",
    "        self._zone_counts = zone_counts\n",
    "        self._zone_counter = zone_counter\n",
    "        \n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self._result, item)\n",
    "\n",
    "    @property\n",
    "    def image_overlay(self):\n",
    "        if self._zone_counter is not None:\n",
    "            return self._zone_counter.annotate(self._result.image_overlay.copy(), self.zone_counts)            \n",
    "        else:           \n",
    "            return self._result.image_overlay\n",
    "\n",
    "def predict_stream(model,\n",
    "                   video_source,\n",
    "                   zone_counter=None                   \n",
    "                   ):\n",
    "      \n",
    "    model.image_backend = \"opencv\"\n",
    "    model.input_numpy_colorspace = \"BGR\"\n",
    "    do_zone_count = zone_counter is not None\n",
    "    zone_counts=None\n",
    "\n",
    "    with degirum_tools.open_video_stream(video_source) as stream:\n",
    "        for inference_result in model.predict_batch(degirum_tools.video_source(stream)):            \n",
    "            if len(res.results):\n",
    "                detections = sv.Detections(np.array([obj[\"bbox\"] for obj in res.results]), \n",
    "                                            confidence=np.array([obj[\"score\"] for obj in res.results]),\n",
    "                                            class_id=np.array([obj[\"category_id\"] for obj in res.results]))\n",
    "                labels = [ f\"{model.label_dictionary[class_id]} {confidence:0.2f}\"\n",
    "                          for _, _, confidence, class_id, track_id in detections\n",
    "                          ]                \n",
    "            else:\n",
    "                detections=sv.Detections.empty()\n",
    "                labels=[]\n",
    "            if do_zone_count:\n",
    "                zone_counts=zone_counter.count(res,detections)\n",
    "                           \n",
    "            yield PredictStreamResult(res,detections,labels, zone_counter,zone_counts)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = zoo.load_model(model_name, overlay_line_width=1)\n",
    "\n",
    "# define polygon zone coordinates\n",
    "polygons = [\n",
    "    [[10, 50], [600, 50], [600, 400], [10, 400]],\n",
    "]\n",
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "# Drag zone by left mouse button to move zone\n",
    "# Drag zone corners by right mouse button to adjust zone shape\n",
    "with degirum_tools.Display(\"AI Camera\") as display:\n",
    "    # create zone counter\n",
    "    zone_counter = ZoneCounter(polygons,\n",
    "                               zone_colors=[sv.Color(255,255,0)],\n",
    "                               triggering_position=ZoneCounter.CENTER,\n",
    "                               window_name=display.window_name,  # attach display window for interactive zone adjustment\n",
    "                               )\n",
    "\n",
    "    # do AI predictions on video stream\n",
    "    for result in predict_stream( model, video_source, zone_counter=zone_counter):\n",
    "        display.show(result.image_overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_counter.annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.Color()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import numpy as np\n",
    "byte_tracker = sv.ByteTrack()\n",
    "box_annotator = sv.BoundingBoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "video_source='../../images/people_720p.mp4'\n",
    "frame=0\n",
    "with degirum_tools.Display(\"AI Camera\") as display: \n",
    "    for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "        if len(res.results):\n",
    "            detections = sv.Detections(np.array([obj[\"bbox\"] for obj in res.results]), \n",
    "                                        confidence=np.array([obj[\"score\"] for obj in res.results]),\n",
    "                                        class_id=np.array([obj[\"category_id\"] for obj in res.results]))\n",
    "            detections = byte_tracker.update_with_detections(detections)\n",
    "            labels = [ f\"#{tracker_id} {model.label_dictionary[class_id]} {confidence:0.2f}\"\n",
    "            for _, _, confidence, class_id, tracker_id in detections\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            detections=sv.Detections.empty()\n",
    "            labels=[]\n",
    "        annotated_frame = box_annotator.annotate(scene=res.image.copy(),\n",
    "                                                detections=detections                                                      \n",
    "                                                )\n",
    "        annotated_labeled_frame = label_annotator.annotate(scene=annotated_frame, \n",
    "                                                        detections=detections,\n",
    "                                                        labels=labels\n",
    "                                                        )\n",
    "        display.show(annotated_labeled_frame)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sv.DetectionDataset.from_coco(images_directory_path='C:/Users/ShashiChilappagari/Documents/Python_Scripts/Datasets/coco/images/val2017/',\n",
    "                                   annotations_path='C:/Users/ShashiChilappagari/Documents/Python_Scripts/Datasets/coco/annotations/instances_val2017.json'\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = zoo.load_model(model_name, overlay_line_width=1)\n",
    "\n",
    "# define polygon zone coordinates\n",
    "polygons = [\n",
    "    [[10, 50], [600, 50], [600, 400], [10, 400]],\n",
    "]\n",
    "\n",
    "# AI prediction loop\n",
    "# Press 'x' or 'q' to stop\n",
    "# Drag zone by left mouse button to move zone\n",
    "# Drag zone corners by right mouse button to adjust zone shape\n",
    "with degirum_tools.Display(\"AI Camera\") as display:\n",
    "    # create zone counter\n",
    "    zone_counter = degirum_tools.ZoneCounter(\n",
    "        polygons,\n",
    "        class_list=[\"person\"],\n",
    "        triggering_position=degirum_tools.ZoneCounter.CENTER,\n",
    "        window_name=display.window_name,  # attach display window for interactive zone adjustment\n",
    "    )\n",
    "\n",
    "    # do AI predictions on video stream\n",
    "    for inference_result in degirum_tools.predict_stream(\n",
    "        model, video_source, zone_counter=zone_counter\n",
    "    ):\n",
    "        display.show(inference_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (supervision)",
   "language": "python",
   "name": "supervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
