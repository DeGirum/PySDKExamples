{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f07a3d",
   "metadata": {},
   "source": [
    "## Multi Object Tracking sample\n",
    "This notebook is an example how to perform object detection with multi-object tracking (MOT) from a video file to count vehicle traffic.\n",
    "The **ByteTracker** is used for multi-object tracking (see https://github.com/ifzhang/ByteTrack)\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. [DeGirum Cloud Platform](https://cs.degirum.com),\n",
    "1. DeGirum-hosted AI server node shared via Peer-to-Peer VPN,\n",
    "1. AI server node hosted by you in your local network,\n",
    "1. AI server running on your local machine,\n",
    "1. DeGirum ORCA accelerator directly installed on your local machine.\n",
    "\n",
    "To try different options, you just need to change the `inference_option` in the code below.\n",
    "\n",
    "### This sample uses the following external packages, which need to be installed:\n",
    "1. **cython_bbox**: `pip install -e git+https://github.com/samson-wang/cython_bbox.git#egg=cython-bbox`\n",
    "1. **lap**: `pip install lap`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863f7c-973f-4df8-84a2-7a9ef9f4a7bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specify where do you want to run your inferences and dataset parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_option = 1 # <<< change it according to your needs selecting from the list in the header comment\n",
    "\n",
    "# model name to be used for inference\n",
    "model_name = \"yolo_v5s_coco--512x512_quant_n2x_orca_1\"\n",
    "\n",
    "# input video file\n",
    "input_filename = 'images/Traffic.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056cfa5-479a-4bc5-9a15-52ed3ebae89a",
   "metadata": {},
   "source": [
    "### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6121d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import numpy as np\n",
    "import mytools, cv2\n",
    "from pathlib import Path\n",
    "import IPython.display\n",
    "lap = mytools.import_optional_package(\"lap\")\n",
    "cython_bbox = mytools.import_optional_package(\"cython_bbox\")\n",
    "from mot.byte_tracker import BYTETracker\n",
    "from mot.basetrack import BaseTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0ef34-f06d-494f-8883-b9c87aae5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to model zoo according to selected inference option\n",
    "zoo = mytools.connect_model_zoo(inference_option)\n",
    "\n",
    "# load object detection model\n",
    "model = zoo.load_model(model_name)\n",
    "\n",
    "# set model parameters\n",
    "model.image_backend = 'opencv' # select OpenCV backend: needed to have overlay image in OpenCV format\n",
    "model.input_numpy_colorspace = 'BGR'\n",
    "model.overlay_show_probabilities = True\n",
    "model.overlay_line_width = 1\n",
    "model._model_parameters.InputImgFmt = ['JPEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a07f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video input and output\n",
    "orig_path = Path(input_filename)\n",
    "ann_path = orig_path.with_name(orig_path.stem + \"_annotated\" + orig_path.suffix) # this is output path, you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8155b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dict_dot_notation(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "# return bool, check line intersect\n",
    "def intersect(a, b, c, d):\n",
    "    s = (a[0] - b[0]) * (c[1] - a[1]) - (a[1] - b[1]) * (c[0] - a[0])\n",
    "    t = (a[0] - b[0]) * (d[1] - a[1]) - (a[1] - b[1]) * (d[0] - a[0])\n",
    "    if s * t > 0:\n",
    "        return False\n",
    "    s = (c[0] - d[0]) * (a[1] - c[1]) - (c[1] - d[1]) * (a[0] - c[0])\n",
    "    t = (c[0] - d[0]) * (b[1] - c[1]) - (c[1] - d[1]) * (b[0] - c[0])\n",
    "    if s * t > 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI prediction loop\n",
    "# this loop make a video to image folder with suffix \"_annotated\"\n",
    "with mytools.open_video_stream(input_filename) as stream:\n",
    "    \n",
    "    image_w = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    image_h = int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # count line (x, y)\n",
    "    line_start = (0, 2 * image_h // 3)\n",
    "    line_end = (image_w, line_start[1])\n",
    "\n",
    "    # counters for each direction\n",
    "    left = right = top = bottom = 0\n",
    "    \n",
    "    BaseTrack._count = 0 # reset track counter\n",
    "    \n",
    "    with mytools.Display(\"MoT\") as display, \\\n",
    "         mytools.open_video_writer(str(ann_path), image_w, image_h) as writer:\n",
    "    \n",
    "        fps = 30 # you can specify input video FPS if you want\n",
    "        tracker = BYTETracker(\n",
    "            args=dict_dot_notation({\n",
    "                'track_thresh': 0.3,\n",
    "                'track_buffer': fps * 2,\n",
    "                'match_thresh': 0.8,\n",
    "                'mot20': False,\n",
    "            }),\n",
    "            frame_rate=fps\n",
    "        )\n",
    "        timeout_count_dict = {}\n",
    "        is_counted_dict = {}\n",
    "        trail_dict = {}\n",
    "        timeout_count_initial = fps\n",
    "\n",
    "        progress = mytools.Progress(int(stream.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        for batch_result in model.predict_batch(mytools.video_source(stream, report_error=False)):\n",
    "            # object detection\n",
    "            results = batch_result.results\n",
    "            bboxes = np.zeros((len(results), 5))\n",
    "            image = batch_result.image\n",
    "\n",
    "            # byte track\n",
    "            for index, result in enumerate(results):\n",
    "                bbox = np.array(result.get('bbox', [0, 0, 0, 0]))\n",
    "                score = result.get('score', 0)\n",
    "                bbox_and_score = np.append(bbox, score)\n",
    "                bboxes[index] = bbox_and_score\n",
    "\n",
    "            online_targets = tracker.update(bboxes, (1, 1), (1, 1))\n",
    "            online_target_set = set([])\n",
    "\n",
    "            # tracking start or continue\n",
    "            for target in online_targets:\n",
    "                tid = str(target.track_id)\n",
    "                online_target_set.add(str(tid))\n",
    "\n",
    "                box = tuple(map(int, target.tlbr)) # x1 y1 x2 y2\n",
    "                center = tuple(map(int, target.tlwh_to_xyah(target.tlwh)[:2]))\n",
    "                if trail_dict.get(tid, None) is None:\n",
    "                    trail_dict[tid] = []\n",
    "                if is_counted_dict.get(tid, None) is None:\n",
    "                    is_counted_dict[tid] = False\n",
    "                if not is_counted_dict[tid] and len(trail_dict[tid]) > 1:\n",
    "                    trail_start = trail_dict[tid][0]\n",
    "                    trail_end = center\n",
    "                    is_cross = intersect(line_start, line_end, trail_start, trail_end)\n",
    "                    if is_cross:\n",
    "                        if trail_start[0] > trail_end[0]:\n",
    "                            left += 1\n",
    "                        if trail_start[0] < trail_end[0]:\n",
    "                            right += 1\n",
    "                        if trail_start[1] < trail_end[1]:\n",
    "                            top += 1\n",
    "                        if trail_start[1] > trail_end[1]:\n",
    "                            bottom += 1\n",
    "                        is_counted_dict[tid] = True\n",
    "                trail_dict[tid].append(center)\n",
    "                timeout_count_dict[tid] = timeout_count_initial\n",
    "                if len(trail_dict[tid]) > 1:\n",
    "                    cv2.polylines(image, [np.array(trail_dict[tid])], False, (255, 255, 0))\n",
    "                mytools.Display.put_text(image, tid, (box[0], box[3]), (255,255,255), (0,0,0), cv2.FONT_HERSHEY_PLAIN)\n",
    "                cv2.rectangle(image, box[0:2], box[2:4], color=(0, 255, 0), thickness=1)\n",
    "                cv2.drawMarker(image, center, (255, 255, 0), markerType=cv2.MARKER_CROSS)\n",
    "                \n",
    "\n",
    "            # tracking terminate\n",
    "            for tid in set(timeout_count_dict.keys()) - online_target_set:\n",
    "                timeout_count_dict[tid] -= 1\n",
    "                if timeout_count_dict[tid] == 0:\n",
    "                    del timeout_count_dict[tid], is_counted_dict[tid], trail_dict[tid]\n",
    "\n",
    "            text = 'Top={} Bottom={} Left={} Right={}'.format(top, bottom, left, right)\n",
    "            mytools.Display.put_text(image, text, (image_w // 3, 0), (255,255,255), (0,0,0), cv2.FONT_HERSHEY_PLAIN)\n",
    "            cv2.line(image, line_start, line_end, (0, 255, 0))\n",
    "\n",
    "            writer.write(image)\n",
    "            display.show(image)\n",
    "            progress.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211246e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display result\n",
    "IPython.display.Video(filename=str(ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original video\n",
    "IPython.display.Video(filename=str(orig_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3d043-ee43-4a07-be6e-2f286fc6c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af213c-d26d-4187-aa96-e79676ff9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.FONT_HERSHEY_SIMPLEX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "50de14cfcf8437409e83adf65890e3e47263b30fd21ab1f0117168323be0df4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
