{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438aa03a",
   "metadata": {},
   "source": [
    "## This notebook is an example of how to pipeline two models. \n",
    "A video stream from a local camera is processed by the person detection model. The person detection results are then processed by the pose detection model, one person bounding box at a time. Combined result is then displayed.\n",
    "\n",
    "This example uses `mystreams` streaming toolkit.\n",
    "\n",
    "**Access to camera is required to run this sample.**\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. [DeGirum Cloud Platform](https://cs.degirum.com),\n",
    "1. DeGirum-hosted AI server node shared via Peer-to-Peer VPN,\n",
    "1. AI server node hosted by you in your local network,\n",
    "1. AI server running on your local machine,\n",
    "1. DeGirum ORCA accelerator directly installed on your local machine.\n",
    "\n",
    "To try different options, you just need to change the `inference_option` in the code below.\n",
    "\n",
    "The script needs either a web camera or local camera connected to the machine running this code. The camera index or URL needs to be specified either in the code below by assigning `camera_id` or in .env file by defining `CAMERA_ID` variable and assigning `camera_id = None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0f95d-7fd0-43b1-b94a-4835357d10a5",
   "metadata": {},
   "source": [
    "### Specify where do you want to run your inferences and camera index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b37dddf-3f5f-40b0-8bd9-409ae37bc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_option = 1  # <<< change it according to your needs selecting from the list in the header comment\n",
    "camera_id = 0         # camera index or URL; 0 to use default local camera, None to take from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dc838-9d2b-4806-b48c-67912b4fdebe",
   "metadata": {},
   "source": [
    "### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d4cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg # import DeGirum PySDK\n",
    "import mytools, cv2\n",
    "from mystreams import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983841ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference option = 'DeGirum Cloud Platform' at dgcps://cs.degirum.com\n"
     ]
    }
   ],
   "source": [
    "# connect to model zoo according to selected inference option\n",
    "zoo = mytools.connect_model_zoo(inference_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33012fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "people_det_model = zoo.load_model(\"yolo_v5s_person_det--512x512_quant_n2x_orca_1\")\n",
    "pose_model = zoo.load_model(\"mobilenet_v1_posenet_coco_keypoints--353x481_quant_n2x_orca_1\")\n",
    "\n",
    "# adjust pose model properties\n",
    "pose_model.output_pose_threshold = 0.2 # lower threshold\n",
    "pose_model.overlay_line_width = 1\n",
    "pose_model.overlay_alpha = 1\n",
    "pose_model.overlay_show_labels = False\n",
    "pose_model.overlay_color = (255, 0, 0)\n",
    "\n",
    "# adjust people model properties\n",
    "people_det_model.overlay_show_probabilities = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac9c92-0100-4c13-aafd-d81f65176958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pose detection gizmo (in mystreams terminology)\n",
    "class PoseDetectionGizmo(AiGizmoBase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._cur_result = None\n",
    "        \n",
    "    def on_result(self, result):\n",
    "        \n",
    "        # here result.info contains StreamData object used for AI inference (because AiGizmoBase does it this way);\n",
    "        # and result.info.meta contains metainfo dictionary placed by AiObjectDetectionCroppingGizmo, \n",
    "        # because in our pipeline it is connected as a source of this gizmo\n",
    "        meta = result.info.meta\n",
    "        if \"original_result\" in meta: # new frame comes\n",
    "            if self._cur_result is not None:\n",
    "                # send previous frame\n",
    "                self.send_result(StreamData(self._cur_result.image, self._cur_result))                \n",
    "            \n",
    "            # save first pose result object at the beginning of new frame in order to accumulate all poses into it\n",
    "            self._cur_result = result\n",
    "            # replace original image with full annotated image which came from person detector to show person boxes as well as poses\n",
    "            self._cur_result._input_image = meta[\"original_result\"].image_overlay            \n",
    "        \n",
    "        if \"cropped_index\" in meta and \"cropped_result\" in meta:            \n",
    "            # convert pose coordinates to back to original image\n",
    "            box = meta[\"cropped_result\"][\"bbox\"]\n",
    "            for r in result.results:\n",
    "                for p in r['landmarks']:\n",
    "                    p['landmark'][0] += box[0]\n",
    "                    p['landmark'][1] += box[1]\n",
    "                        \n",
    "            if self._cur_result != result:\n",
    "                # accumulate all other detected poses into current result object\n",
    "                self._cur_result._inference_results += result.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf099d64-6388-4aa7-aee5-767615d8996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition started\n",
      "Successfully opened video stream '0'\n",
      "Composition stopped\n"
     ]
    }
   ],
   "source": [
    "# create composition object\n",
    "c = Composition();\n",
    "\n",
    "# create gizmos adding them to composition\n",
    "source = c.add(VideoSourceGizmo(camera_id)) # video source\n",
    "people_detection = c.add(AiObjectDetectionCroppingGizmo([\"person\"], people_det_model)) # people detection gizmo, which outputs cropped image for each detected person\n",
    "pose_detection = c.add(PoseDetectionGizmo(pose_model)) # pose detection gizmo\n",
    "display = c.add(VideoDisplayGizmo(\"Person Poses\", show_ai_overlay=True, show_fps=True)) # display\n",
    "\n",
    "# connect gizmos to create pipeline\n",
    "source >> people_detection >> pose_detection >> display\n",
    "\n",
    "# start execution of composition \n",
    "c.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15f39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
