{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Source and Multi-Model AI Inference\n",
    "This notebook is an example how to perform AI inferences of multiple models processing multiple video streams.\n",
    "Each video stream is fed to every model. Each model processes frames from every video stream in multiplexing manner.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you just need to uncomment **one** of the lines in the code below.\n",
    "\n",
    "You also need to specify your cloud API access token, cloud zoo URLs, and AI server hostname in [env.ini](env.ini) file, \n",
    "located in the same directory as this notebook.\n",
    "\n",
    "The script may use a web camera(s) or local camera(s) connected to the machine running this code.\n",
    "The camera index or URL needs to be specified either in the code below by assigning `camera_id` or \n",
    "in [env.ini](env.ini) file by defining `CAMERA_ID` variable and assigning `camera_id = None`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify video sources and AI model names here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of video sources: it can be video file names, indexes or local cameras or IP camera URLs\n",
    "video_sources = [\n",
    "    \"Images/Traffic.mp4\",\n",
    "    \"Images/TrafficHD.mp4\",\n",
    "]\n",
    "\n",
    "# list of AI models to use for inferences\n",
    "# NOTE: they should have the same input size\n",
    "model_names = [\n",
    "    \"yolo_v5s_hand_det--512x512_quant_n2x_orca_1\",\n",
    "    \"yolo_v5s_face_det--512x512_quant_n2x_orca_1\",\n",
    "    \"yolo_v5n_car_det--512x512_quant_n2x_orca_1\",\n",
    "    \"yolo_v5s_person_det--512x512_quant_n2x_orca_1\",\n",
    "]\n",
    "\n",
    "# when True, we drop video frames in case when AI performance is not enough to work in real time\n",
    "# when False, we buffer video frames to keep up with AI performance\n",
    "allow_frame_drop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, mytools\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "target = dg.CLOUD # <-- on the Cloud Platform\n",
    "# target = mytools.get_ai_server_hostname() # <-- on AI Server deployed in your LAN\n",
    "# target = dg.LOCAL # <-- on ORCA accelerator installed on this computer\n",
    "\n",
    "# connect to AI inference engine getting zoo URL and token from env.ini file\n",
    "zoo = dg.connect(target, mytools.get_cloud_zoo_url(), mytools.get_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mystreams\n",
    "\n",
    "c = mystreams.Composition()\n",
    "\n",
    "batch_size = len(\n",
    "    video_sources\n",
    ")  # set AI server batch size equal to the # of video sources for lowest latency\n",
    "\n",
    "# create PySDK AI model objects\n",
    "models = []\n",
    "for mi, model_name in enumerate(model_names):\n",
    "    model = zoo.load_model(model_name)\n",
    "    model.input_image_format = \"JPEG\"\n",
    "    model.measure_time = True\n",
    "    model.eager_batch_size = batch_size\n",
    "    model.frame_queue_depth = batch_size\n",
    "    models.append(model)\n",
    "\n",
    "# check that all models have the same input configuration\n",
    "models_have_same_input = True\n",
    "for model in models[1:]:\n",
    "    if (\n",
    "        type(model._preprocessor) != type(models[0]._preprocessor)\n",
    "        or model.model_info.InputH != models[0].model_info.InputH\n",
    "        or model.model_info.InputW != models[0].model_info.InputW\n",
    "    ):\n",
    "        models_have_same_input = False\n",
    "\n",
    "resizers = []\n",
    "\n",
    "# create video sources and image resizers\n",
    "# (we use separate resizers to do resize only once per source when possible, to improve performance),\n",
    "# connect each resizer to corresponding video source\n",
    "for src in video_sources:\n",
    "    source = c.add(mystreams.VideoSourceGizmo(src))\n",
    "    if models_have_same_input:\n",
    "        resizer = c.add(\n",
    "            mystreams.AiPreprocessGizmo(\n",
    "                models[0], stream_depth=2, allow_drop=allow_frame_drop\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        resizer = c.add(mystreams.FanoutGizmo(allow_drop=allow_frame_drop))\n",
    "\n",
    "    resizer.connect_to(source)  # connect resizer to video source\n",
    "    resizers.append(resizer)\n",
    "\n",
    "# create result combiner\n",
    "combiner = c.add(mystreams.AiResultCombiningGizmo(len(models)))\n",
    "\n",
    "# create multi-input detector gizmos,\n",
    "# connect each detector gizmo to every resizer gizmo,\n",
    "# connect result combiner gizmo to each detector gizmo\n",
    "for mi, model in enumerate(models):\n",
    "\n",
    "    # create AI gizmo (aka detector) from the model\n",
    "    detector = c.add(\n",
    "        mystreams.AiSimpleGizmo(model, stream_depth=2, inp_cnt=len(video_sources))\n",
    "    )\n",
    "\n",
    "    # connect detector gizmo to each resizer gizmo\n",
    "    for fi, resizer in enumerate(resizers):\n",
    "        detector.connect_to(resizer, fi)\n",
    "\n",
    "    # connect result combiner gizmo to detector gizmo\n",
    "    combiner.connect_to(detector, mi)\n",
    "\n",
    "# create multi-window video multiplexing display gizmo\n",
    "# and connect it to combiner gizmo\n",
    "win_captions = [f\"Camera #{i}: {str(src)}\" for i, src in enumerate(video_sources)]\n",
    "display = c.add(\n",
    "    mystreams.VideoDisplayGizmo(\n",
    "        win_captions, show_ai_overlay=True, show_fps=True, multiplex=True\n",
    "    )\n",
    ")\n",
    "display.connect_to(combiner)\n",
    "\n",
    "# start composition\n",
    "c.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
